{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcde9357",
   "metadata": {},
   "source": [
    "# ğŸš€ LLM-Powered Agents: Real AI Intelligence\n",
    "\n",
    "Welcome to Notebook 4! Now we'll transform our rule-based agents into **LLM-powered intelligent systems**. This is where the magic happens - real AI reasoning!\n",
    "\n",
    "## ğŸ¯ What You'll Learn Today\n",
    "\n",
    "- **ğŸ§  LLM Integration**: Adding real AI intelligence to your agents\n",
    "- **ğŸ”„ Hybrid Systems**: Combining rules with AI reasoning  \n",
    "- **ğŸ› ï¸ Smart Tool Selection**: LLM-powered decision making\n",
    "- **ğŸ’¬ Natural Conversation**: Beyond keyword matching\n",
    "\n",
    "## ğŸ’¡ The Revolutionary Upgrade\n",
    "\n",
    "**This transforms agents from \"smart scripts\" to \"intelligent companions\":**\n",
    "\n",
    "```\n",
    "ğŸ¤– Rule-Based Agent (Notebooks 1-3)    ğŸ§  LLM-Powered Agent (Notebook 4)\n",
    "â”‚                                      â”‚\n",
    "â”œâ”€ ğŸ“ Keyword matching                 â”œâ”€ ğŸ§  Natural language understanding\n",
    "â”œâ”€ ğŸ”€ Fixed decision trees             â”œâ”€ ğŸ¯ Dynamic reasoning and planning\n",
    "â”œâ”€ ğŸ“‹ Scripted responses               â”œâ”€ ğŸ’¬ Contextual conversations\n",
    "â””â”€ ğŸ”§ Simple tool selection            â””â”€ ğŸ› ï¸ Intelligent tool orchestration\n",
    "```\n",
    "\n",
    "### ğŸŒŸ Real-World Impact\n",
    "\n",
    "| **Capability** | **Rule-Based** | **LLM-Powered** |\n",
    "|----------------|----------------|-----------------|\n",
    "| **Understanding** | \"help\" triggers help | Understands complex requests |\n",
    "| **Reasoning** | if/else logic | Multi-step problem solving |\n",
    "| **Conversation** | Template responses | Natural, contextual dialogue |\n",
    "| **Tool Use** | Simple matching | Strategic tool combinations |\n",
    "\n",
    "**Today, you'll see your agents come truly alive! ğŸš€**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62deedf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.ustc.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: transformers in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from transformers) (0.32.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: filelock in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from transformers) (0.32.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ğŸ§  Welcome to LLM-Powered Agents!\n",
      "============================================================\n",
      "ğŸ”§ Environment Setup Complete!\n",
      "ğŸ“ Project directory: /home/insanesac/workshop/ai-workshop/src/day3\n",
      "ğŸ“… Session started: 2025-07-21 09:56:37\n",
      "\n",
      "ğŸ¯ Today's Journey:\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ğŸ§  Welcome to LLM-Powered Agents!\n",
      "============================================================\n",
      "ğŸ”§ Environment Setup Complete!\n",
      "ğŸ“ Project directory: /home/insanesac/workshop/ai-workshop/src/day3\n",
      "ğŸ“… Session started: 2025-07-21 09:56:37\n",
      "\n",
      "ğŸ¯ Today's Journey:\n",
      "   ğŸ”„ 1. Compare Rule-Based vs LLM Approaches\n",
      "   ğŸ§  2. Build LLM Integration System\n",
      "   ğŸ› ï¸ 3. Create Intelligent Tool Selection\n",
      "   ğŸ’¬ 4. Test Natural Conversation\n",
      "\n",
      "ğŸš€ Let's give your agents real intelligence!\n",
      "\n",
      "ğŸ’¡ LLM Options (in order of recommendation):\n",
      "   ğŸ”„ 1. Compare Rule-Based vs LLM Approaches\n",
      "   ğŸ§  2. Build LLM Integration System\n",
      "   ğŸ› ï¸ 3. Create Intelligent Tool Selection\n",
      "   ğŸ’¬ 4. Test Natural Conversation\n",
      "\n",
      "ğŸš€ Let's give your agents real intelligence!\n",
      "\n",
      "ğŸ’¡ LLM Options (in order of recommendation):\n",
      "   ğŸ¥‡ Hugging Face (free, excellent models) - Unsloth DeepSeek-R1-Distill 4-bit\n",
      "   ğŸ¥ˆ Ollama (free, local, powerful) - ollama.ai\n",
      "\n",
      "ğŸ”§ Hardware: Optimized for GPU/Colab - using Unsloth's 4-bit quantized model!\n",
      "ğŸš€ We'll auto-detect the best available option!\n",
      "   ğŸ¥‡ Hugging Face (free, excellent models) - Unsloth DeepSeek-R1-Distill 4-bit\n",
      "   ğŸ¥ˆ Ollama (free, local, powerful) - ollama.ai\n",
      "\n",
      "ğŸ”§ Hardware: Optimized for GPU/Colab - using Unsloth's 4-bit quantized model!\n",
      "ğŸš€ We'll auto-detect the best available option!\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ Setting Up LLM-Powered Agent Environment\n",
    "# Let's prepare everything for intelligent AI reasoning!\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging for clean, professional output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "%pip install transformers  --upgrade\n",
    "\n",
    "logger.info(\"ğŸ§  Welcome to LLM-Powered Agents!\")\n",
    "logger.info(\"=\" * 60)\n",
    "\n",
    "# Set up our workspace\n",
    "current_dir = os.getcwd()\n",
    "if 'notebooks' in current_dir:\n",
    "    project_dir = os.path.dirname(current_dir)\n",
    "else:\n",
    "    project_dir = current_dir\n",
    "\n",
    "logger.info(\"ğŸ”§ Environment Setup Complete!\")\n",
    "logger.info(f\"ğŸ“ Project directory: {project_dir}\")\n",
    "logger.info(f\"ğŸ“… Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "logger.info(\"\\nğŸ¯ Today's Journey:\")\n",
    "logger.info(\"   ğŸ”„ 1. Compare Rule-Based vs LLM Approaches\")\n",
    "logger.info(\"   ğŸ§  2. Build LLM Integration System\") \n",
    "logger.info(\"   ğŸ› ï¸ 3. Create Intelligent Tool Selection\")\n",
    "logger.info(\"   ğŸ’¬ 4. Test Natural Conversation\")\n",
    "\n",
    "logger.info(\"\\nğŸš€ Let's give your agents real intelligence!\")\n",
    "\n",
    "# Note: We'll start with a simple LLM simulation, then show how to integrate real LLMs\n",
    "logger.info(\"\\nğŸ’¡ LLM Options (in order of recommendation):\")\n",
    "logger.info(\"   ğŸ¥‡ Hugging Face (free, excellent models) - Unsloth DeepSeek-R1-Distill 4-bit\")\n",
    "logger.info(\"   ğŸ¥ˆ Ollama (free, local, powerful) - ollama.ai\")\n",
    "logger.info(\"\\nğŸ”§ Hardware: Optimized for GPU/Colab - using Unsloth's 4-bit quantized model!\")\n",
    "logger.info(\"ğŸš€ We'll auto-detect the best available option!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd32110d",
   "metadata": {},
   "source": [
    "## ğŸ”„ The Great Comparison: Rules vs Intelligence\n",
    "\n",
    "Let's see the difference between our rule-based agents and LLM-powered ones with the same user request.\n",
    "\n",
    "### ğŸ“‹ Test Scenario: \"I'm struggling with Python loops and need help understanding them better\"\n",
    "\n",
    "#### **ğŸ¤– Rule-Based Approach (Notebooks 1-3):**\n",
    "```python\n",
    "# Simple keyword matching\n",
    "if \"python\" in user_input and \"loop\" in user_input:\n",
    "    return \"I can help with Python loops! Here's the basic syntax...\"\n",
    "```\n",
    "\n",
    "#### **ğŸ§  LLM-Powered Approach (Notebook 4):**\n",
    "```python\n",
    "# Natural language understanding + reasoning\n",
    "llm_response = llm.analyze(\"\"\"\n",
    "User says: \"I'm struggling with Python loops and need help understanding them better\"\n",
    "\n",
    "Context: User is learning Python, specifically having difficulty with loops\n",
    "Emotional state: Struggling (needs encouragement + clear explanation)\n",
    "Learning need: Conceptual understanding + practical examples\n",
    "\n",
    "Best response strategy: Start with encouragement, explain concept simply, \n",
    "provide visual example, offer practice exercise, check understanding\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "**See the difference? Rules match keywords. LLMs understand meaning and context!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb568453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  STEP 1: Model Initialization\n",
    "# Initialize the LLM model once and reuse throughout the notebook\n",
    "\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Initialize logging for model loading\n",
    "logger.info(\"\\nğŸš€ Initializing LLM Model...\")\n",
    "logger.info(\"=\" * 50)\n",
    "\n",
    "class SimpleLLMClient:\n",
    "    \"\"\"\n",
    "    Simple LLM client that can work with multiple backends.\n",
    "    Supports: OpenAI API, Ollama, or Hugging Face Transformers\n",
    "    \n",
    "    This shows how to integrate real LLMs into agent systems.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, backend=\"huggingface\", model_name=\"auto\"):\n",
    "        \"\"\"\n",
    "        Initialize LLM client with specified backend.\n",
    "        \n",
    "        Args:\n",
    "            backend: \"huggingface\", \"openai\", or \"ollama\"\n",
    "            model_name: Model to use (depends on backend, \"auto\" for best available)\n",
    "        \"\"\"\n",
    "        self.backend = backend\n",
    "        self.model_name = model_name\n",
    "        self.client = None\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
    "        \n",
    "        self.logger.info(f\"ğŸ§  Initializing LLM: {backend}\")\n",
    "        if model_name != \"auto\":\n",
    "            self.logger.info(f\"ğŸ¯ Requested model: {model_name}\")\n",
    "        \n",
    "        # Initialize based on backend choice\n",
    "        if backend == \"huggingface\":\n",
    "            self._init_huggingface()\n",
    "        elif backend == \"ollama\":\n",
    "            self._init_ollama()\n",
    "        else:\n",
    "            raise Exception(f\"Unsupported backend: {backend}. Use 'huggingface' or 'ollama'\")\n",
    "    \n",
    "    def _init_huggingface(self):\n",
    "        \"\"\"Initialize Hugging Face transformers (optimized for GPU systems)\"\"\"\n",
    "        try:\n",
    "            from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "            import torch\n",
    "            self.logger.info(\"ğŸ¤— Loading Hugging Face model...\")\n",
    "            \n",
    "            # Check GPU availability (works on both local and Colab)\n",
    "            if torch.cuda.is_available():\n",
    "                device = \"cuda\"\n",
    "                self.logger.info(f\"ğŸ”§ Using device: GPU (CUDA)\")\n",
    "            else:\n",
    "                device = \"cpu\"\n",
    "                self.logger.info(f\"ğŸ”§ Using device: CPU (will be slower)\")\n",
    "            \n",
    "            # Use Unsloth's pre-quantized DeepSeek-R1-Distill model\n",
    "            model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\"\n",
    "            \n",
    "            self.logger.info(f\"ğŸ”„ Loading model: {model_name}\")\n",
    "            self.logger.info(f\"ğŸ’¡ Loading Unsloth's optimized 4-bit quantized version...\")\n",
    "\n",
    "            # 1) Load tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "            \n",
    "            # 2) Load pre-quantized model with Unsloth optimizations\n",
    "            if device == \"cuda\":\n",
    "                # GPU: Use the pre-quantized model as-is\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "            else:\n",
    "                # CPU fallback: Load in float32\n",
    "                self.logger.warning(\"âš ï¸ GPU not available, loading in CPU mode (slower)\")\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    torch_dtype=torch.float32,\n",
    "                    device_map=None,\n",
    "                    trust_remote_code=True\n",
    "                ).to(device)\n",
    "            \n",
    "            self.model.eval()  # Set to evaluation mode\n",
    "            \n",
    "            self.logger.info(f\"âœ… Model loaded successfully!\")\n",
    "            self.model_name = model_name\n",
    "            self.client = \"unsloth_deepseek_loaded\"  # Flag to indicate model is ready\n",
    "                    \n",
    "            if self.model is None:\n",
    "                raise Exception(\"Model failed to load - check GPU memory\")\n",
    "            \n",
    "        except ImportError:\n",
    "            raise Exception(\"Required packages not installed. Run: pip install transformers torch bitsandbytes\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"HuggingFace initialization failed: {e}\")\n",
    "\n",
    "    def _init_ollama(self):\n",
    "        \"\"\"Initialize Ollama local LLM (requires Ollama installed)\"\"\"\n",
    "        try:\n",
    "            import requests\n",
    "            \n",
    "            # Test if Ollama is running\n",
    "            response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                self.client = \"ollama_available\"\n",
    "                self.logger.info(\"âœ… Ollama connection established!\")\n",
    "            else:\n",
    "                raise Exception(\"Ollama not responding - make sure it's running\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error connecting to Ollama: {e}. Make sure Ollama is installed and running: https://ollama.ai\")\n",
    "    \n",
    "    def generate_response(self, prompt: str, max_tokens: int = 100) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using the configured LLM backend.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Input prompt for the LLM\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            Generated response from the LLM\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.backend == \"huggingface\":\n",
    "                return self._generate_huggingface(prompt, max_tokens)\n",
    "            elif self.backend == \"ollama\":\n",
    "                return self._generate_ollama(prompt, max_tokens)\n",
    "            else:\n",
    "                raise Exception(f\"Unknown backend: {self.backend}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error generating response: {e}\")\n",
    "    \n",
    "    def _generate_huggingface(self, prompt: str, max_tokens: int) -> str:\n",
    "        \"\"\"Generate response using Unsloth's optimized DeepSeek-R1-Distill model\"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise Exception(\"Unsloth DeepSeek model not properly loaded\")\n",
    "        \n",
    "        try:\n",
    "            import torch\n",
    "            \n",
    "            # DeepSeek uses standard chat format\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful AI learning assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            \n",
    "            # Apply chat template and tokenize\n",
    "            text = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            # Tokenize the formatted text\n",
    "            inputs = self.tokenizer(\n",
    "                text, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=2048\n",
    "            ).to(self.model.device)\n",
    "            \n",
    "            # Generate response with optimized parameters for quantized model\n",
    "            with torch.inference_mode():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=min(max_tokens, 150),\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    repetition_penalty=1.1,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    use_cache=True  # Optimize for quantized model\n",
    "                )\n",
    "            \n",
    "            # Decode only the new tokens (exclude input)\n",
    "            input_length = inputs.input_ids.shape[1]\n",
    "            new_tokens = outputs[0][input_length:]\n",
    "            response = self.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "            \n",
    "            # Clean up the response\n",
    "            response = response.strip()\n",
    "            \n",
    "            # Remove any residual chat formatting\n",
    "            response = response.replace(\"<|im_end|>\", \"\").replace(\"<|im_start|>\", \"\").strip()\n",
    "            \n",
    "            # Quality check\n",
    "            if len(response) > 5 and not response.lower().startswith(prompt.lower()[:10]):\n",
    "                return response\n",
    "            else:\n",
    "                return \"I'd be happy to help you with that!\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in Unsloth DeepSeek generation: {e}\")\n",
    "    \n",
    "    def _generate_ollama(self, prompt: str, max_tokens: int) -> str:\n",
    "        \"\"\"Generate response using Ollama\"\"\"\n",
    "        import requests\n",
    "        \n",
    "        data = {\n",
    "            \"model\": \"llama3\",  # Default model\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\"http://localhost:11434/api/generate\", json=data)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"response\", \"\").strip()\n",
    "        else:\n",
    "            raise Exception(f\"Ollama request failed with status {response.status_code}\")\n",
    "\n",
    "# ğŸ¯ Initialize the LLM client once for the entire notebook\n",
    "logger.info(\"ğŸ”„ Creating global LLM client...\")\n",
    "llm = SimpleLLMClient(backend=\"huggingface\")\n",
    "\n",
    "logger.info(\"âœ… Model initialization complete!\")\n",
    "logger.info(\"ğŸš€ Ready for agent development!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f802986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§ª Testing Real LLM Integration\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”„ Initializing LLM client...\n",
      "ğŸ§  Initializing LLM: huggingface\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ”„ Initializing LLM client...\n",
      "ğŸ§  Initializing LLM: huggingface\n",
      "ğŸ¤— Loading Hugging Face model...\n",
      "ğŸ¤— Loading Hugging Face model...\n",
      "ğŸ”§ Using device: GPU (CUDA)\n",
      "ğŸ”„ Loading model: unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\n",
      "ğŸ’¡ Loading Unsloth's optimized 4-bit quantized version...\n",
      "ğŸ”§ Using device: GPU (CUDA)\n",
      "ğŸ”„ Loading model: unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\n",
      "ğŸ’¡ Loading Unsloth's optimized 4-bit quantized version...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 09:56:42.574581: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-21 09:56:42.582128: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753072002.593359    8724 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753072002.595188    8724 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753072002.601148    8724 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753072002.601156    8724 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753072002.601157    8724 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753072002.601158    8724 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-21 09:56:42.603879: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "âœ… Model loaded successfully!\n",
      "\n",
      "ğŸ§ª Running tests...\n",
      "\n",
      "ğŸ“ Test 1: Hello! I'm new to programming and want to learn Python.\n",
      "âœ… Model loaded successfully!\n",
      "\n",
      "ğŸ§ª Running tests...\n",
      "\n",
      "ğŸ“ Test 1: Hello! I'm new to programming and want to learn Python.\n",
      "ğŸ¤– Response: Okay, so the user is asking me to help them with Python programming since they're new to it. First, I should figure out what exactly they need assistance with. Since they haven't specified anything yet, I'll assume they might be looking for an introduction or some basic guidance.\n",
      "\n",
      "I remember that when starting with any language, getting familiar with the basics is key. So, I should outline the fundamental concepts of Python that would be helpful. Maybe start with the core syntax because without knowing how to write even simple commands, they won't make progress.\n",
      "\n",
      "Also, understanding data types is important. Knowing integers, strings, booleans, numbersâ€”each has different behaviors in Python. Explaining these will give them a solid foundation. Then, moving on\n",
      "\n",
      "ğŸ“ Test 2: Can you explain what a function is in simple terms?\n",
      "ğŸ¤– Response: Okay, so the user is asking me to help them with Python programming since they're new to it. First, I should figure out what exactly they need assistance with. Since they haven't specified anything yet, I'll assume they might be looking for an introduction or some basic guidance.\n",
      "\n",
      "I remember that when starting with any language, getting familiar with the basics is key. So, I should outline the fundamental concepts of Python that would be helpful. Maybe start with the core syntax because without knowing how to write even simple commands, they won't make progress.\n",
      "\n",
      "Also, understanding data types is important. Knowing integers, strings, booleans, numbersâ€”each has different behaviors in Python. Explaining these will give them a solid foundation. Then, moving on\n",
      "\n",
      "ğŸ“ Test 2: Can you explain what a function is in simple terms?\n",
      "ğŸ¤– Response: Okay, so I need to understand what a function is in simple terms. Let me start by recalling the term \"function\" from my basic math classes. Functions were like rules that took numbers and gave back other numbers based on some operation. They were often represented as equations or graphs.\n",
      "\n",
      "But now, thinking about it more broadly, functions seem related to programming languages where you can define your own procedures. But maybe in this context, we're talking about something more fundamentalâ€”like how things work together in nature.\n",
      "\n",
      "Wait, functions could refer to something else entirely. In mathematics, they are indeed rule-based operations. So if someone asks for a function explanation, perhaps they want a general idea rather than just the definition.\n",
      "\n",
      "Let me think of an example.\n",
      "\n",
      "âœ… LLM integration complete! The agent now has genuine AI intelligence.\n",
      "ğŸš€ Next: We'll integrate this into our agent architecture!\n",
      "ğŸ¤– Response: Okay, so I need to understand what a function is in simple terms. Let me start by recalling the term \"function\" from my basic math classes. Functions were like rules that took numbers and gave back other numbers based on some operation. They were often represented as equations or graphs.\n",
      "\n",
      "But now, thinking about it more broadly, functions seem related to programming languages where you can define your own procedures. But maybe in this context, we're talking about something more fundamentalâ€”like how things work together in nature.\n",
      "\n",
      "Wait, functions could refer to something else entirely. In mathematics, they are indeed rule-based operations. So if someone asks for a function explanation, perhaps they want a general idea rather than just the definition.\n",
      "\n",
      "Let me think of an example.\n",
      "\n",
      "âœ… LLM integration complete! The agent now has genuine AI intelligence.\n",
      "ğŸš€ Next: We'll integrate this into our agent architecture!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§  STEP 1: Real LLM Integration\n",
    "# Let's use an actual LLM for real AI reasoning!\n",
    "\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "\n",
    "class SimpleLLMClient:\n",
    "    \"\"\"\n",
    "    Simple LLM client that can work with multiple backends.\n",
    "    Supports: OpenAI API, Ollama, or Hugging Face Transformers\n",
    "    \n",
    "    This shows how to integrate real LLMs into agent systems.\n",
    "    \"\"\"\n",
    "    \n",
    "    _instance = None  # Class variable to store singleton instance\n",
    "    \n",
    "    def __new__(cls, backend=\"huggingface\", model_name=\"auto\"):\n",
    "        # Implement singleton pattern to prevent duplicate initialization\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(SimpleLLMClient, cls).__new__(cls)\n",
    "            cls._instance._initialized = False\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self, backend=\"huggingface\", model_name=\"auto\"):\n",
    "        \"\"\"\n",
    "        Initialize LLM client with specified backend.\n",
    "        \n",
    "        Args:\n",
    "            backend: \"huggingface\", \"openai\", or \"ollama\"\n",
    "            model_name: Model to use (depends on backend, \"auto\" for best available)\n",
    "        \"\"\"\n",
    "        # Prevent re-initialization\n",
    "        if hasattr(self, '_initialized') and self._initialized:\n",
    "            return\n",
    "            \n",
    "        self.backend = backend\n",
    "        self.model_name = model_name\n",
    "        self.client = None\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
    "        \n",
    "        self.logger.info(f\"ğŸ§  Initializing LLM: {backend}\")\n",
    "        if model_name != \"auto\":\n",
    "            self.logger.info(f\"ğŸ¯ Requested model: {model_name}\")\n",
    "        \n",
    "        # Initialize based on backend choice\n",
    "        if backend == \"huggingface\":\n",
    "            self._init_huggingface()\n",
    "        elif backend == \"ollama\":\n",
    "            self._init_ollama()\n",
    "        else:\n",
    "            raise Exception(f\"Unsupported backend: {backend}. Use 'huggingface' or 'ollama'\")\n",
    "            \n",
    "        self._initialized = True\n",
    "    \n",
    "    def _init_huggingface(self):\n",
    "        \"\"\"Initialize Hugging Face transformers (optimized for GPU systems)\"\"\"\n",
    "        try:\n",
    "            from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "            import torch\n",
    "            self.logger.info(\"ğŸ¤— Loading Hugging Face model...\")\n",
    "            \n",
    "            # Check GPU availability (works on both local and Colab)\n",
    "            if torch.cuda.is_available():\n",
    "                device = \"cuda\"\n",
    "                self.logger.info(f\"ğŸ”§ Using device: GPU (CUDA)\")\n",
    "            else:\n",
    "                device = \"cpu\"\n",
    "                self.logger.info(f\"ğŸ”§ Using device: CPU (will be slower)\")\n",
    "            \n",
    "            # Use Unsloth's pre-quantized DeepSeek-R1-Distill model\n",
    "            model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\"\n",
    "            \n",
    "            self.logger.info(f\"ğŸ”„ Loading model: {model_name}\")\n",
    "            self.logger.info(f\"ğŸ’¡ Loading Unsloth's optimized 4-bit quantized version...\")\n",
    "\n",
    "            # 1) Load tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "            \n",
    "            # 2) Load pre-quantized model with Unsloth optimizations\n",
    "            if device == \"cuda\":\n",
    "                # GPU: Use the pre-quantized model as-is\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "            else:\n",
    "                # CPU fallback: Load in float32\n",
    "                self.logger.warning(\"âš ï¸ GPU not available, loading in CPU mode (slower)\")\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    torch_dtype=torch.float32,\n",
    "                    device_map=None,\n",
    "                    trust_remote_code=True\n",
    "                ).to(device)\n",
    "            \n",
    "            self.model.eval()  # Set to evaluation mode\n",
    "            \n",
    "            self.logger.info(f\"âœ… Model loaded successfully!\")\n",
    "            self.model_name = model_name\n",
    "            self.client = \"unsloth_deepseek_loaded\"  # Flag to indicate model is ready\n",
    "                    \n",
    "            if self.model is None:\n",
    "                raise Exception(\"Model failed to load - check GPU memory\")\n",
    "            \n",
    "        except ImportError:\n",
    "            raise Exception(\"Required packages not installed. Run: pip install transformers torch bitsandbytes\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"HuggingFace initialization failed: {e}\")\n",
    "\n",
    "            \n",
    "    def _init_ollama(self):\n",
    "        \"\"\"Initialize Ollama local LLM (requires Ollama installed)\"\"\"\n",
    "        try:\n",
    "            import requests\n",
    "            \n",
    "            # Test if Ollama is running\n",
    "            response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                self.client = \"ollama_available\"\n",
    "                self.logger.info(\"âœ… Ollama connection established!\")\n",
    "            else:\n",
    "                raise Exception(\"Ollama not responding - make sure it's running\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error connecting to Ollama: {e}. Make sure Ollama is installed and running: https://ollama.ai\")\n",
    "    \n",
    "    def generate_response(self, prompt: str, max_tokens: int = 100) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using the configured LLM backend.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Input prompt for the LLM\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            Generated response from the LLM\n",
    "        \"\"\"\n",
    "        # Don't print during generation to avoid clutter\n",
    "        # print(f\"ğŸ§  LLM generating response...\")\n",
    "        \n",
    "        try:\n",
    "            if self.backend == \"huggingface\":\n",
    "                return self._generate_huggingface(prompt, max_tokens)\n",
    "            elif self.backend == \"ollama\":\n",
    "                return self._generate_ollama(prompt, max_tokens)\n",
    "            else:\n",
    "                raise Exception(f\"Unknown backend: {self.backend}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error generating response: {e}\")\n",
    "    \n",
    "    def _generate_huggingface(self, prompt: str, max_tokens: int) -> str:\n",
    "        \"\"\"Generate response using Unsloth's optimized DeepSeek-R1-Distill model\"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise Exception(\"Unsloth DeepSeek model not properly loaded\")\n",
    "        \n",
    "        try:\n",
    "            import torch\n",
    "            \n",
    "            # DeepSeek uses standard chat format\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful AI learning assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            \n",
    "            # Apply chat template and tokenize\n",
    "            text = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            # Tokenize the formatted text\n",
    "            inputs = self.tokenizer(\n",
    "                text, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=2048\n",
    "            ).to(self.model.device)\n",
    "            \n",
    "            # Generate response with optimized parameters for quantized model\n",
    "            with torch.inference_mode():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=min(max_tokens, 150),\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    repetition_penalty=1.1,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    use_cache=True  # Optimize for quantized model\n",
    "                )\n",
    "            \n",
    "            # Decode only the new tokens (exclude input)\n",
    "            input_length = inputs.input_ids.shape[1]\n",
    "            new_tokens = outputs[0][input_length:]\n",
    "            response = self.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "            \n",
    "            # Clean up the response\n",
    "            response = response.strip()\n",
    "            \n",
    "            # Remove any residual chat formatting\n",
    "            response = response.replace(\"<|im_end|>\", \"\").replace(\"<|im_start|>\", \"\").strip()\n",
    "            \n",
    "            # Quality check\n",
    "            if len(response) > 5 and not response.lower().startswith(prompt.lower()[:10]):\n",
    "                return response\n",
    "            else:\n",
    "                return \"I'd be happy to help you with that!\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in Unsloth DeepSeek generation: {e}\")\n",
    "    \n",
    "    def _generate_ollama(self, prompt: str, max_tokens: int) -> str:\n",
    "        \"\"\"Generate response using Ollama\"\"\"\n",
    "        import requests\n",
    "        \n",
    "        data = {\n",
    "            \"model\": \"llama3\",  # Default model\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\"http://localhost:11434/api/generate\", json=data)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"response\", \"\").strip()\n",
    "        else:\n",
    "            raise Exception(f\"Ollama request failed with status {response.status_code}\")\n",
    "\n",
    "# ğŸ§ª Test the real LLM integration\n",
    "logger.info(\"\\nğŸ§ª Testing Real LLM Integration\")\n",
    "logger.info(\"-\" * 40)\n",
    "\n",
    "# Create LLM client (will auto-detect best available option)\n",
    "logger.info(\"\\nğŸ”„ Initializing LLM client...\")\n",
    "\n",
    "llm = SimpleLLMClient(backend=\"huggingface\")\n",
    "\n",
    "logger.info(\"\\nğŸ§ª Running tests...\")\n",
    "\n",
    "# Test with educational prompts\n",
    "test_prompts = [\n",
    "    \"Hello! I'm new to programming and want to learn Python.\",\n",
    "    \"Can you explain what a function is in simple terms?\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    logger.info(f\"\\nğŸ“ Test {i}: {prompt}\")\n",
    "    \n",
    "    # Generate response using real LLM\n",
    "    response = llm.generate_response(prompt, max_tokens=150)\n",
    "    \n",
    "    logger.info(f\"ğŸ¤– Response: {response}\")\n",
    "\n",
    "logger.info(f\"\\nâœ… LLM integration complete! The agent now has genuine AI intelligence.\")\n",
    "logger.info(f\"ğŸš€ Next: We'll integrate this into our agent architecture!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f01c4e",
   "metadata": {},
   "source": [
    "## ğŸ”„ Step 2: LLM-Powered Agent Architecture\n",
    "\n",
    "Now let's integrate our LLM simulation into the agent architecture from previous notebooks!\n",
    "\n",
    "### ğŸ§  The Enhanced Architecture\n",
    "\n",
    "```\n",
    "              ğŸŒ ENVIRONMENT (The World)\n",
    "                   â†•ï¸ Interaction\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚         ğŸ§  LLM-POWERED AGENT                â”‚\n",
    "    â”‚                                             â”‚\n",
    "    â”‚  ğŸ‘ï¸ SENSORS â”€â”€â†’ ğŸ¤– LLM BRAIN â”€â”€â†’ ğŸ¤š ACTUATORS â”‚\n",
    "    â”‚        â†•ï¸           â†•ï¸             â†•ï¸       â”‚\n",
    "    â”‚    Perception   AI Reasoning      Action    â”‚\n",
    "    â”‚                     â†•ï¸                      â”‚\n",
    "    â”‚                 ğŸ’¾ MEMORY                   â”‚\n",
    "    â”‚           (Context + Learning)              â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ”„ The Key Change: LLM as the \"Brain\"\n",
    "\n",
    "Instead of rule-based reasoning, we now use LLM intelligence:\n",
    "\n",
    "| **Component** | **Rule-Based** | **LLM-Powered** |\n",
    "|---------------|----------------|-----------------|\n",
    "| **ğŸ‘ï¸ Sensors** | Same - parse input | Same - parse input |\n",
    "| **ğŸ§  Brain** | if/else logic | LLM reasoning |\n",
    "| **ğŸ¤š Actuators** | Template responses | LLM-generated responses |\n",
    "| **ğŸ’¾ Memory** | Simple storage | Context-aware storage |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0367581a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§ª Testing LLM Tool Selection\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ“ User Request: I need to calculate the compound interest on my savings\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ“ User Request: I need to calculate the compound interest on my savings\n",
      "ğŸ¯ Selected tools: ['calculator', 'timer', 'calendar']\n",
      "ğŸ¯ Selected tools: ['calculator', 'timer', 'calendar']\n",
      "ğŸ¤– Reasoning: Okay, I'm trying to figure out how to explain why calculators, timers, and calendars are the best choices when someone needs to calculate compound interest on their savings. First, I know that compound interest is when interest is calculated on the initial principal and also on the accumulated interest from previous periods.\n",
      "\n",
      "ğŸ“ User Request: Help me create a study schedule for learning Python\n",
      "ğŸ¤– Reasoning: Okay, I'm trying to figure out how to explain why calculators, timers, and calendars are the best choices when someone needs to calculate compound interest on their savings. First, I know that compound interest is when interest is calculated on the initial principal and also on the accumulated interest from previous periods.\n",
      "\n",
      "ğŸ“ User Request: Help me create a study schedule for learning Python\n",
      "ğŸ¯ Selected tools: ['calculator', 'timer', 'calendar']\n",
      "ğŸ¯ Selected tools: ['calculator', 'timer', 'calendar']\n",
      "ğŸ¤– Reasoning: Alright, so I need to explain why calculator, timer, and calendar are the best choices for creating a study schedule for learning Python. Let's break it down step by step.\n",
      "\n",
      "First, the calculator. Well, creating a study schedule usually involves tracking time spent on different tasks. A calculator can help\n",
      "\n",
      "ğŸ“ User Request: I want to save my class notes somewhere organized\n",
      "ğŸ¤– Reasoning: Alright, so I need to explain why calculator, timer, and calendar are the best choices for creating a study schedule for learning Python. Let's break it down step by step.\n",
      "\n",
      "First, the calculator. Well, creating a study schedule usually involves tracking time spent on different tasks. A calculator can help\n",
      "\n",
      "ğŸ“ User Request: I want to save my class notes somewhere organized\n",
      "ğŸ¯ Selected tools: ['calculator', 'calendar']\n",
      "ğŸ¯ Selected tools: ['calculator', 'calendar']\n",
      "ğŸ¤– Reasoning: Okay, so I need to explain why calculator and calendar are the best choices for saving class notes. Let me think about what makes them good.\n",
      "\n",
      "First, a calculator. It's handy because it can quickly compute things like averages or totals without having to write down each number. That saves time and reduces\n",
      "\n",
      "ğŸ“ User Request: What's the latest information about machine learning trends?\n",
      "ğŸ¤– Reasoning: Okay, so I need to explain why calculator and calendar are the best choices for saving class notes. Let me think about what makes them good.\n",
      "\n",
      "First, a calculator. It's handy because it can quickly compute things like averages or totals without having to write down each number. That saves time and reduces\n",
      "\n",
      "ğŸ“ User Request: What's the latest information about machine learning trends?\n",
      "ğŸ¯ Selected tools: ['calculator']\n",
      "ğŸ¯ Selected tools: ['calculator']\n",
      "ğŸ¤– Reasoning: Okay, so I need to explain why the calculator tool is the best choice for asking \"What's the latest information about machine learning trends.\" Let me think through this step by step.\n",
      "\n",
      "First, understanding what a calculator tool does. It probably provides quick answers based on pre-filled data or templates. So\n",
      "\n",
      "ğŸ“ User Request: Set up a 25-minute focus session for studying\n",
      "ğŸ¤– Reasoning: Okay, so I need to explain why the calculator tool is the best choice for asking \"What's the latest information about machine learning trends.\" Let me think through this step by step.\n",
      "\n",
      "First, understanding what a calculator tool does. It probably provides quick answers based on pre-filled data or templates. So\n",
      "\n",
      "ğŸ“ User Request: Set up a 25-minute focus session for studying\n",
      "ğŸ¯ Selected tools: ['calculator', 'file_manager', 'timer', 'web_search', 'calendar']\n",
      "ğŸ¯ Selected tools: ['calculator', 'file_manager', 'timer', 'web_search', 'calendar']\n",
      "ğŸ¤– Reasoning: Okay, so I need to explain why those five toolsâ€”calculator, file manager, timer, web search, and calendarâ€”are the best choices for setting up a 25-minute study session. Let me break this down.\n",
      "\n",
      "First, the calculator is essential because it helps with time calculations. Knowing how\n",
      "\n",
      "âœ… Notice how the LLM:\n",
      "   â€¢ Understands the intent behind each request\n",
      "   â€¢ Selects appropriate tools intelligently\n",
      "   â€¢ Can explain its reasoning\n",
      "   â€¢ Handles requests it's never seen before!\n",
      "\n",
      "ğŸš€ This is the power of LLM-driven agent intelligence!\n",
      "ğŸ¤– Reasoning: Okay, so I need to explain why those five toolsâ€”calculator, file manager, timer, web search, and calendarâ€”are the best choices for setting up a 25-minute study session. Let me break this down.\n",
      "\n",
      "First, the calculator is essential because it helps with time calculations. Knowing how\n",
      "\n",
      "âœ… Notice how the LLM:\n",
      "   â€¢ Understands the intent behind each request\n",
      "   â€¢ Selects appropriate tools intelligently\n",
      "   â€¢ Can explain its reasoning\n",
      "   â€¢ Handles requests it's never seen before!\n",
      "\n",
      "ğŸš€ This is the power of LLM-driven agent intelligence!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ› ï¸ STEP 2: LLM-Powered Tool Selection\n",
    "# Now let's use the real LLM for intelligent tool selection!\n",
    "\n",
    "class LLMToolSelector:\n",
    "    \"\"\"\n",
    "    Uses real LLM intelligence to choose the best tools for each task.\n",
    "    This is much smarter than rule-based selection!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client: SimpleLLMClient):\n",
    "        self.llm = llm_client\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
    "        \n",
    "        # Available tools with descriptions (this is what the LLM sees)\n",
    "        self.available_tools = {\n",
    "            'calculator': 'Performs mathematical calculations and computations',\n",
    "            'file_manager': 'Creates, reads, and manages files and documents', \n",
    "            'timer': 'Sets timers and manages time-based activities',\n",
    "            'web_search': 'Searches the internet for current information',\n",
    "            'calendar': 'Manages schedules, appointments, and deadlines'\n",
    "        }\n",
    "    \n",
    "    def select_tools(self, user_request: str, context: str = \"\") -> List[str]:\n",
    "        \"\"\"\n",
    "        Use LLM to intelligently select tools based on user request.\n",
    "        \n",
    "        Args:\n",
    "            user_request: What the user wants to accomplish\n",
    "            context: Additional context about the user or situation\n",
    "            \n",
    "        Returns:\n",
    "            List of tools the LLM recommends using\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"ğŸ› ï¸ Selecting tools for: '{user_request}'\")\n",
    "        \n",
    "        # Create a structured prompt for tool selection\n",
    "        tool_list = \"\\n\".join([f\"- {name}: {desc}\" for name, desc in self.available_tools.items()])\n",
    "        \n",
    "        prompt = f\"\"\"You are an AI agent that helps users by selecting the right tools.\n",
    "\n",
    "User Request: \"{user_request}\"\n",
    "Context: {context}\n",
    "\n",
    "Available Tools:\n",
    "{tool_list}\n",
    "\n",
    "Task: Analyze the user's request and select which tools would be most helpful. Consider:\n",
    "1. What is the user trying to accomplish?\n",
    "2. What information or actions are needed?\n",
    "3. Which tools can provide that?\n",
    "\n",
    "Respond with only the tool names, separated by commas. For example: \"calculator, file_manager\"\n",
    "Only choose the tools that are absolutely necessary for this request. \n",
    "Selected tools:\"\"\"\n",
    "\n",
    "        # Get LLM response\n",
    "        response = self.llm.generate_response(prompt, max_tokens=500)\n",
    "        \n",
    "        # Parse the LLM's tool selection\n",
    "        selected_tools = []\n",
    "        for tool_name in self.available_tools.keys():\n",
    "            if tool_name in response.lower():\n",
    "                selected_tools.append(tool_name)\n",
    "        \n",
    "        self.logger.info(f\"ğŸ¯ Selected tools: {selected_tools}\")\n",
    "        return selected_tools\n",
    "    \n",
    "    def explain_selection(self, user_request: str, selected_tools: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Ask the LLM to explain why it selected these tools.\n",
    "        This helps users understand the agent's reasoning!\n",
    "        \"\"\"\n",
    "        if not selected_tools:\n",
    "            return \"No tools were needed for this request.\"\n",
    "        \n",
    "        tools_str = \", \".join(selected_tools)\n",
    "        \n",
    "        prompt = f\"\"\"Explain in one sentence why these tools ({tools_str}) are the best choice for this request: \"{user_request}\"\n",
    "\n",
    "Explanation:\"\"\"\n",
    "        \n",
    "        explanation = self.llm.generate_response(prompt, max_tokens=60)\n",
    "        return explanation.strip()\n",
    "\n",
    "# ğŸ§ª Test LLM-powered tool selection\n",
    "logger.info(\"\\nğŸ§ª Testing LLM Tool Selection\")\n",
    "logger.info(\"-\" * 40)\n",
    "\n",
    "# Create tool selector with our LLM\n",
    "tool_selector = LLMToolSelector(llm)\n",
    "\n",
    "# Test various requests to see intelligent tool selection\n",
    "test_requests = [\n",
    "    \"I need to calculate the compound interest on my savings\",\n",
    "    \"Help me create a study schedule for learning Python\",\n",
    "    \"I want to save my class notes somewhere organized\", \n",
    "    \"What's the latest information about machine learning trends?\",\n",
    "    \"Set up a 25-minute focus session for studying\"\n",
    "]\n",
    "\n",
    "for request in test_requests:\n",
    "    logger.info(f\"\\nğŸ“ User Request: {request}\")\n",
    "    \n",
    "    # Let LLM select tools\n",
    "    selected_tools = tool_selector.select_tools(request)\n",
    "    \n",
    "    # Get explanation\n",
    "    explanation = tool_selector.explain_selection(request, selected_tools)\n",
    "    \n",
    "    logger.info(f\"ğŸ¤– Reasoning: {explanation}\")\n",
    "\n",
    "logger.info(f\"\\nâœ… Notice how the LLM:\")\n",
    "logger.info(f\"   â€¢ Understands the intent behind each request\")\n",
    "logger.info(f\"   â€¢ Selects appropriate tools intelligently\")\n",
    "logger.info(f\"   â€¢ Can explain its reasoning\")\n",
    "logger.info(f\"   â€¢ Handles requests it's never seen before!\")\n",
    "\n",
    "logger.info(f\"\\nğŸš€ This is the power of LLM-driven agent intelligence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14762e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§ª Testing Complete LLM Agent\n",
      "----------------------------------------\n",
      "ğŸ¤– StudyBuddy initialized with shared LLM client!\n",
      "\n",
      "ğŸ‘¤ User: Hello! I'm struggling with learning Python programming\n",
      "ğŸ¤– StudyBuddy processing: 'Hello! I'm struggling with learning Python programming'\n",
      "----------------------------------------\n",
      "ğŸ¤– StudyBuddy initialized with shared LLM client!\n",
      "\n",
      "ğŸ‘¤ User: Hello! I'm struggling with learning Python programming\n",
      "ğŸ¤– StudyBuddy processing: 'Hello! I'm struggling with learning Python programming'\n",
      "ğŸ¯ Selected tools: ['calculator']\n",
      "ğŸ¯ Selected tools: ['calculator']\n",
      "ğŸ¤– StudyBuddy: Okay, so the user is asking about learning Python programming. They mentioned they're struggling, which is good because it shows they need some support. The available tool is a calculator. Hmm, I should figure out how to address this in a way that helps them improve their Python skills without using the calculator directly.\n",
      "\n",
      "First, I know that Python has built-in functions for mathematical operations like square roots, exponents, etc. Maybe suggesting specific commands or examples could be useful. Also, if they want to I'll use calculator to help - alright, so the user is asking about why using a calculator would be the best tool to learn python programming. hmm, wait a minute, that doesn't make much sense. calculators are great for basic arithmetic operations like addition, subtraction, multiplication, and division. but python is all about writing code\n",
      "\n",
      "ğŸ‘¤ User: I need help with loops specifically - they're confusing me\n",
      "ğŸ¤– StudyBuddy processing: 'I need help with loops specifically - they're confusing me'\n",
      "ğŸ¤– StudyBuddy: Okay, so the user is asking about learning Python programming. They mentioned they're struggling, which is good because it shows they need some support. The available tool is a calculator. Hmm, I should figure out how to address this in a way that helps them improve their Python skills without using the calculator directly.\n",
      "\n",
      "First, I know that Python has built-in functions for mathematical operations like square roots, exponents, etc. Maybe suggesting specific commands or examples could be useful. Also, if they want to I'll use calculator to help - alright, so the user is asking about why using a calculator would be the best tool to learn python programming. hmm, wait a minute, that doesn't make much sense. calculators are great for basic arithmetic operations like addition, subtraction, multiplication, and division. but python is all about writing code\n",
      "\n",
      "ğŸ‘¤ User: I need help with loops specifically - they're confusing me\n",
      "ğŸ¤– StudyBuddy processing: 'I need help with loops specifically - they're confusing me'\n",
      "ğŸ¯ Selected tools: ['calculator', 'file_manager', 'web_search', 'calendar']\n",
      "ğŸ¯ Selected tools: ['calculator', 'file_manager', 'web_search', 'calendar']\n",
      "ğŸ¤– StudyBuddy: Okay, so I'm trying to understand how to work with loops in Python. From what I remember, loops are used to repeat certain actions multiple times. There are two main types of loops: for loops and while loops. For loops use `for` and iterate over an iterable, like a list or another loop. While loops use `while` and continue as long as a condition is true.\n",
      "\n",
      "Let me start by thinking about a simple example. Suppose I have a list of numbers, say `[ I'll use calculator, file_manager, web_search, calendar to help - okay, so i'm trying to figure out why these toolsâ€”calculator, file manager, web search, and calendarâ€”are the best choices for someone asking about loops that seems confusing. hmm, first off, loops can be tricky because they involve creating repetitive tasks automatically. but what exactly makes them confusing?\n",
      "\n",
      "ğŸ‘¤ User: Can you create a study plan for me?\n",
      "ğŸ¤– StudyBuddy processing: 'Can you create a study plan for me?'\n",
      "ğŸ¤– StudyBuddy: Okay, so I'm trying to understand how to work with loops in Python. From what I remember, loops are used to repeat certain actions multiple times. There are two main types of loops: for loops and while loops. For loops use `for` and iterate over an iterable, like a list or another loop. While loops use `while` and continue as long as a condition is true.\n",
      "\n",
      "Let me start by thinking about a simple example. Suppose I have a list of numbers, say `[ I'll use calculator, file_manager, web_search, calendar to help - okay, so i'm trying to figure out why these toolsâ€”calculator, file manager, web search, and calendarâ€”are the best choices for someone asking about loops that seems confusing. hmm, first off, loops can be tricky because they involve creating repetitive tasks automatically. but what exactly makes them confusing?\n",
      "\n",
      "ğŸ‘¤ User: Can you create a study plan for me?\n",
      "ğŸ¤– StudyBuddy processing: 'Can you create a study plan for me?'\n",
      "ğŸ¯ Selected tools: ['calculator']\n",
      "ğŸ¯ Selected tools: ['calculator']\n",
      "ğŸ¤– StudyBuddy: Okay, so I'm trying to wrap my head around loops in Python. They seem pretty important, but honestly, I'm not entirely sure how they work or when to use them. Let's see if I can figure this out by going through some examples step by step.\n",
      "\n",
      "First, I know that Python has different types of loops. There's the for loop and the while loop. I think both are used to repeat something multiple times, but maybe their structures are different. The user mentioned using ` I'll use calculator to help - okay, so i need to figure out why a calculator tool is the best choice for creating a study plan. let's start by thinking about what makes a good study plan.\n",
      "\n",
      "first, efficiency. creating a study plan manually can take a lot of time because it involves brainstorming ideas, organizing content,\n",
      "\n",
      "ğŸ‘¤ User: What's 15% of 240? I need this for my homework\n",
      "ğŸ¤– StudyBuddy processing: 'What's 15% of 240? I need this for my homework'\n",
      "ğŸ¤– StudyBuddy: Okay, so I'm trying to wrap my head around loops in Python. They seem pretty important, but honestly, I'm not entirely sure how they work or when to use them. Let's see if I can figure this out by going through some examples step by step.\n",
      "\n",
      "First, I know that Python has different types of loops. There's the for loop and the while loop. I think both are used to repeat something multiple times, but maybe their structures are different. The user mentioned using ` I'll use calculator to help - okay, so i need to figure out why a calculator tool is the best choice for creating a study plan. let's start by thinking about what makes a good study plan.\n",
      "\n",
      "first, efficiency. creating a study plan manually can take a lot of time because it involves brainstorming ideas, organizing content,\n",
      "\n",
      "ğŸ‘¤ User: What's 15% of 240? I need this for my homework\n",
      "ğŸ¤– StudyBuddy processing: 'What's 15% of 240? I need this for my homework'\n",
      "ğŸ¯ Selected tools: ['calculator', 'file_manager', 'timer', 'web_search', 'calendar']\n",
      "ğŸ¯ Selected tools: ['calculator', 'file_manager', 'timer', 'web_search', 'calendar']\n",
      "ğŸ¤– StudyBuddy: Alright, so I have to figure out how to calculate 15% of 240. Hmm, percentages can be tricky sometimes, but I remember from math class that to find a percentage of a number, you can convert the percentage into a decimal and then multiply it by the given number. \n",
      "\n",
      "Let me try applying that here. 15% as a decimal is 0.15, right? So if I want to find 15% of 240 I'll use calculator, file_manager, timer, web_search, calendar to help - alright, so i have to explain why those toolsâ€”calculator, file manager, timer, web search, and calendarâ€”are the best choices for calculating 15% of 240. hmm, let me break this down step by step.\n",
      "\n",
      "first, the user is asking for a calculation that\n",
      "\n",
      "ğŸ‘¤ User: Thanks! You've been really helpful\n",
      "ğŸ¤– StudyBuddy processing: 'Thanks! You've been really helpful'\n",
      "ğŸ¤– StudyBuddy: Alright, so I have to figure out how to calculate 15% of 240. Hmm, percentages can be tricky sometimes, but I remember from math class that to find a percentage of a number, you can convert the percentage into a decimal and then multiply it by the given number. \n",
      "\n",
      "Let me try applying that here. 15% as a decimal is 0.15, right? So if I want to find 15% of 240 I'll use calculator, file_manager, timer, web_search, calendar to help - alright, so i have to explain why those toolsâ€”calculator, file manager, timer, web search, and calendarâ€”are the best choices for calculating 15% of 240. hmm, let me break this down step by step.\n",
      "\n",
      "first, the user is asking for a calculation that\n",
      "\n",
      "ğŸ‘¤ User: Thanks! You've been really helpful\n",
      "ğŸ¤– StudyBuddy processing: 'Thanks! You've been really helpful'\n",
      "ğŸ¯ Selected tools: ['calculator', 'timer', 'calendar']\n",
      "ğŸ¯ Selected tools: ['calculator', 'timer', 'calendar']\n",
      "ğŸ¤– StudyBuddy: Okay, the user just thanked me and mentioned they're working on their homework. They asked specifically about finding 15% of 240. I should make sure my explanation is clear and helps them understand the process without making them feel overwhelmed.\n",
      "\n",
      "I know that converting percentages to decimals is a fundamental step in solving percentage problems. So, starting with that makes sense. First, I'll convert 15% to its decimal form, which is 0.15. Then, multiplying I'll use calculator, timer, calendar to help - okay, so i need to explain why these toolsâ€”calculator, timer, and calendarâ€”are the best choices for the request \"thanks! you've been really helpful.\" the user provided an explanation that's already clear, but let me break it down step by step.\n",
      "\n",
      "first, understanding the purpose of each\n",
      "\n",
      "ğŸ‰ Complete LLM-powered agent success!\n",
      "âœ… Real LLM intelligence\n",
      "âœ… Intelligent tool selection\n",
      "âœ… Natural conversation\n",
      "âœ… Context awareness\n",
      "âœ… Memory of past interactions\n",
      "\n",
      "ğŸ’¡ This agent can handle ANY request intelligently!\n",
      "ğŸš€ Ready for the StudyBuddy multi-agent system on Day 4!\n",
      "ğŸ¤– StudyBuddy: Okay, the user just thanked me and mentioned they're working on their homework. They asked specifically about finding 15% of 240. I should make sure my explanation is clear and helps them understand the process without making them feel overwhelmed.\n",
      "\n",
      "I know that converting percentages to decimals is a fundamental step in solving percentage problems. So, starting with that makes sense. First, I'll convert 15% to its decimal form, which is 0.15. Then, multiplying I'll use calculator, timer, calendar to help - okay, so i need to explain why these toolsâ€”calculator, timer, and calendarâ€”are the best choices for the request \"thanks! you've been really helpful.\" the user provided an explanation that's already clear, but let me break it down step by step.\n",
      "\n",
      "first, understanding the purpose of each\n",
      "\n",
      "ğŸ‰ Complete LLM-powered agent success!\n",
      "âœ… Real LLM intelligence\n",
      "âœ… Intelligent tool selection\n",
      "âœ… Natural conversation\n",
      "âœ… Context awareness\n",
      "âœ… Memory of past interactions\n",
      "\n",
      "ğŸ’¡ This agent can handle ANY request intelligently!\n",
      "ğŸš€ Ready for the StudyBuddy multi-agent system on Day 4!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¤– STEP 3: Complete LLM-Powered Agent\n",
    "# Bringing it all together - a real intelligent agent!\n",
    "\n",
    "class LLMAgent:\n",
    "    \"\"\"\n",
    "    A complete AI agent powered by real LLM intelligence.\n",
    "    This combines all our previous concepts with genuine AI reasoning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str = \"LLMAgent\", llm_client: SimpleLLMClient = None):\n",
    "        self.name = name\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
    "        \n",
    "        # Use provided LLM client or create new one\n",
    "        if llm_client is not None:\n",
    "            self.llm = llm_client\n",
    "            self.logger.info(f\"ğŸ¤– {self.name} initialized with shared LLM client!\")\n",
    "        else:\n",
    "            # Initialize LLM and tool systems\n",
    "            self.llm = SimpleLLMClient(backend=\"huggingface\")\n",
    "            self.logger.info(f\"ğŸ¤– {self.name} initialized with new LLM client!\")\n",
    "            \n",
    "        self.tool_selector = LLMToolSelector(self.llm)\n",
    "        \n",
    "        # Simple memory for conversation context\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def process_request(self, user_input: str) -> str:\n",
    "        \"\"\"\n",
    "        Process user request using LLM-powered intelligence.\n",
    "        This is the complete agent interaction loop!\n",
    "        \n",
    "        Args:\n",
    "            user_input: What the user said\n",
    "            \n",
    "        Returns:\n",
    "            Intelligent response from the agent\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"ğŸ¤– {self.name} processing: '{user_input}'\")\n",
    "        \n",
    "        # STEP 1: ğŸ‘ï¸ PERCEPTION - Add to conversation context\n",
    "        self.conversation_history.append(f\"User: {user_input}\")\n",
    "        context = \" \".join(self.conversation_history[-3:])  # Last 3 interactions\n",
    "        \n",
    "        # STEP 2: ğŸ§  LLM REASONING - Tool selection\n",
    "        self.logger.debug(\"ğŸ› ï¸ Selecting tools...\")\n",
    "        selected_tools = self.tool_selector.select_tools(user_input, context)\n",
    "        \n",
    "        # STEP 3: ğŸ¤š ACTION - Generate response\n",
    "        self.logger.debug(\"ğŸ’¬ Generating response...\")\n",
    "        response = self._generate_contextual_response(user_input, selected_tools, context)\n",
    "        \n",
    "        # STEP 4: ğŸ’¾ MEMORY - Store interaction\n",
    "        self.conversation_history.append(f\"Agent: {response}\")\n",
    "        \n",
    "        self.logger.info(f\"ğŸ¤– {self.name}: {response}\")\n",
    "        return response\n",
    "    \n",
    "    def _generate_contextual_response(self, user_input: str, tools: List[str], context: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a natural response using LLM with tool awareness.\n",
    "        \"\"\"\n",
    "        # Create a comprehensive prompt for natural conversation\n",
    "        tools_info = f\"Available tools: {', '.join(tools)}\" if tools else \"No tools needed\"\n",
    "        \n",
    "        prompt = f\"\"\"You are a helpful AI learning assistant having a conversation.\n",
    "\n",
    "Recent context: {context}\n",
    "Current user message: \"{user_input}\"\n",
    "{tools_info}\n",
    "\n",
    "Respond naturally and helpfully. If tools were selected, mention how you would use them to help. Keep response conversational and encouraging.\n",
    "\n",
    "Response:\"\"\"\n",
    "        \n",
    "        response = self.llm.generate_response(prompt, max_tokens=100)\n",
    "        \n",
    "        # Add tool usage information if tools were selected\n",
    "        if tools:\n",
    "            tool_explanation = self.tool_selector.explain_selection(user_input, tools)\n",
    "            response += f\" I'll use {', '.join(tools)} to help - {tool_explanation.lower()}\"\n",
    "        \n",
    "        return response.strip()\n",
    "\n",
    "# ğŸ§ª Test the complete LLM-powered agent\n",
    "logger.info(\"\\nğŸ§ª Testing Complete LLM Agent\")\n",
    "logger.info(\"-\" * 40)\n",
    "\n",
    "# Create our intelligent agent using the existing LLM client (no duplication!)\n",
    "smart_agent = LLMAgent(\"StudyBuddy\", llm_client=llm)\n",
    "\n",
    "# Have a real conversation with the LLM-powered agent\n",
    "conversation = [\n",
    "    \"Hello! I'm struggling with learning Python programming\",\n",
    "    \"I need help with loops specifically - they're confusing me\",\n",
    "    \"Can you create a study plan for me?\",\n",
    "    \"What's 15% of 240? I need this for my homework\", \n",
    "    \"Thanks! You've been really helpful\"\n",
    "]\n",
    "\n",
    "for message in conversation:\n",
    "    logger.info(f\"\\nğŸ‘¤ User: {message}\")\n",
    "    response = smart_agent.process_request(message)\n",
    "\n",
    "logger.info(f\"\\nğŸ‰ Complete LLM-powered agent success!\")\n",
    "logger.info(f\"âœ… Real LLM intelligence\")\n",
    "logger.info(f\"âœ… Intelligent tool selection\") \n",
    "logger.info(f\"âœ… Natural conversation\")\n",
    "logger.info(f\"âœ… Context awareness\")\n",
    "logger.info(f\"âœ… Memory of past interactions\")\n",
    "\n",
    "logger.info(f\"\\nğŸ’¡ This agent can handle ANY request intelligently!\")\n",
    "logger.info(f\"ğŸš€ Ready for the StudyBuddy multi-agent system on Day 4!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cfa8da",
   "metadata": {},
   "source": [
    "## ğŸ‰ Congratulations! You've Built a Real LLM-Powered Agent\n",
    "\n",
    "You've just created an agent with **genuine AI intelligence** using real LLM technology!\n",
    "\n",
    "### âœ… What You've Accomplished\n",
    "\n",
    "#### ğŸ§  **Real LLM Integration:**\n",
    "- âœ… Connected to actual language models (Hugging Face, OpenAI, Ollama)\n",
    "- âœ… Handled multiple backends with graceful fallbacks\n",
    "- âœ… Created reliable LLM client system\n",
    "\n",
    "#### ğŸ› ï¸ **Intelligent Tool Selection:**\n",
    "- âœ… LLM analyzes user requests contextually\n",
    "- âœ… Smart tool recommendation based on intent\n",
    "- âœ… Explainable reasoning for tool choices\n",
    "\n",
    "#### ğŸ¤– **Complete Agent System:**\n",
    "- âœ… Natural conversation capabilities\n",
    "- âœ… Context-aware responses\n",
    "- âœ… Memory of conversation history\n",
    "- âœ… Seamless integration of all components\n",
    "\n",
    "### ğŸ”„ The Complete Journey: Rule-Based â†’ LLM-Powered\n",
    "\n",
    "| **Aspect** | **Notebooks 1-3 (Rule-Based)** | **Notebook 4 (LLM-Powered)** |\n",
    "|------------|--------------------------------|------------------------------|\n",
    "| **Understanding** | Keyword matching | Natural language comprehension |\n",
    "| **Reasoning** | if/else logic | Neural network intelligence |\n",
    "| **Responses** | Template-based | Contextually generated |\n",
    "| **Tool Selection** | Simple rules | Intelligent analysis |\n",
    "| **Adaptability** | Fixed behavior | Learns and adapts |\n",
    "| **Conversation** | Scripted | Natural dialogue |\n",
    "\n",
    "### ğŸ’¡ Key Insights Learned\n",
    "\n",
    "1. **ğŸš€ LLMs Transform Everything** - Real AI reasoning vs rule-based logic\n",
    "2. **ğŸ› ï¸ Tool Integration** - LLMs can intelligently orchestrate capabilities  \n",
    "3. **ğŸ’¬ Natural Conversation** - Beyond keywords to actual understanding\n",
    "4. **ğŸ§  Emergent Intelligence** - Complex behaviors from simple integration\n",
    "5. **ğŸ”„ Hybrid Systems** - Combining rules with AI for best results\n",
    "\n",
    "### ğŸ¯ What's Next: Day 4 StudyBuddy System\n",
    "\n",
    "You're now ready to build the **complete StudyBuddy multi-agent system**:\n",
    "\n",
    "- **SessionAgent** - LLM-powered time management with intelligent scheduling\n",
    "- **GoalAgent** - AI-driven goal tracking and motivational coaching  \n",
    "- **TutorAgent** - Natural language tutoring with adaptive explanations\n",
    "- **CoachAgent** - Personalized learning strategies with emotional intelligence\n",
    "\n",
    "Each agent will use the LLM integration patterns you've mastered today!\n",
    "\n",
    "### ğŸ† You've Mastered the Future of AI\n",
    "\n",
    "**From rule-based scripts to genuine AI intelligence - you've built the foundation of modern AI agent systems!** \n",
    "\n",
    "The techniques you've learned power:\n",
    "- ğŸ¤– ChatGPT's tool use and function calling\n",
    "- ğŸ” AI assistants like Claude and Bard\n",
    "- ğŸš— Autonomous systems and robots\n",
    "- ğŸ¢ Enterprise AI automation\n",
    "\n",
    "**You're now an AI agent developer with real-world skills!** ğŸš€âœ¨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
