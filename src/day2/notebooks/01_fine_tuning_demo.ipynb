{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Language Model\n",
    "\n",
    "## What You'll Learn\n",
    "In this notebook, we will demonstrate how to fine-tune a pre-trained language model on a specific dataset. By the end, you'll understand:\n",
    "- How to prepare data for fine-tuning\n",
    "- How to customize a pre-trained model for your specific use case\n",
    "- How to evaluate and test your fine-tuned model\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python knowledge\n",
    "- Understanding of what language models are\n",
    "- Familiarity with Jupyter notebooks\n",
    "\n",
    "## What is Fine-Tuning?\n",
    "Fine-tuning is the process of taking a pre-trained model (like GPT-2) and training it further on your specific dataset. This helps the model learn to respond in the style and format you want.\n",
    "\n",
    "We'll use the `distilgpt2` model from Hugging Face Transformers library and adapt it to answer campus-related questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "\n",
    "# Optional: Install TensorFlow Keras\n",
    "# %pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "print(\"Setting up the model and tokenizer...\")\n",
    "\n",
    "# Check if GPU is available and set device\n",
    "# GPU training is much faster than CPU, but CPU will work fine for this demo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the tokenizer and model\n",
    "# Tokenizer: Converts text to numbers that the model can understand\n",
    "# Model: The actual neural network that generates text\n",
    "print(\"Loading DistilGPT-2 model...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "\n",
    "# GPT-2 doesn't have a padding token by default, so we need to add one\n",
    "# Padding tokens help us process multiple texts of different lengths together\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "print(f\"‚úì Model loaded successfully!\")\n",
    "print(f\"‚úì Padding token set to: {tokenizer.pad_token}\")\n",
    "print(f\"‚úì Model is on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "We'll load a campus FAQ dataset containing questions and answers. The model will learn to answer questions in the same style as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "print(\"Loading campus FAQ dataset...\")\n",
    "\n",
    "# Load the dataset and extract questions and answers\n",
    "data_path = '../data/campus_faq.json'\n",
    "\n",
    "try:\n",
    "    with open(data_path) as f:\n",
    "        data = json.load(f)\n",
    "    print(\"‚úì Dataset loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: Could not find the dataset file. Please check the path.\")\n",
    "    raise\n",
    "\n",
    "# Extract questions and answers from the nested structure\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for item in data['faq']:\n",
    "    questions.append(item['question'])\n",
    "    answers.append(item['answer'])\n",
    "\n",
    "# Create DataFrame for easier data manipulation\n",
    "df = pd.DataFrame({\n",
    "    'question': questions,\n",
    "    'answer': answers\n",
    "})\n",
    "\n",
    "print(f\"Dataset contains {len(df)} question-answer pairs\")\n",
    "print(\"\\nFirst 3 examples:\")\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"\\nQ: {df.iloc[i]['question']}\")\n",
    "    print(f\"A: {df.iloc[i]['answer']}\")\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Splitting data into training and validation sets...\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data: 80% for training, 20% for validation\n",
    "train_questions, val_questions, train_answers, val_answers = train_test_split(\n",
    "    df['question'].tolist(),\n",
    "    df['answer'].tolist(),# Add this cell right after the training cell to debug the generation issue\n",
    "\n",
    "    test_size=0.2,\n",
    "    random_state=42  # For reproducible results\n",
    ")\n",
    "\n",
    "print(f\"Training examples: {len(train_questions)}\")\n",
    "print(f\"Validation examples: {len(val_questions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Model\n",
    "\n",
    "We'll create a custom dataset class and set up the training process using Hugging Face's Trainer API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 24 examples\n",
      "Created dataset with 6 examples\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "class QADataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A simple dataset class for question-answer pairs.\n",
    "    \n",
    "    This class converts our Q&A data into a format the model can learn from.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, questions, answers, tokenizer, max_length=512):\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        print(f\"Created dataset with {len(questions)} examples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of examples in our dataset\"\"\"\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the question and answer for the given index\n",
    "        question = self.questions[idx]\n",
    "        answer   = self.answers[idx]\n",
    "        \n",
    "        # Combine question and answer into a single string, following the prompt format used for training\n",
    "        full_text = f\"Question: {question} Answer: {answer}{self.tokenizer.eos_token}\"\n",
    "        \n",
    "        # Tokenize the combined text, pad/truncate to max_length, and return as tensors\n",
    "        enc = self.tokenizer(\n",
    "            full_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Squeeze to remove the batch dimension\n",
    "        input_ids      = enc['input_ids'].squeeze()\n",
    "        attention_mask = enc['attention_mask'].squeeze()\n",
    "        \n",
    "        # Create labels for language modeling\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        # Mask out the question part so that loss is only computed on the answer\n",
    "        question_prefix = self.tokenizer.encode(f\"Question: {question} Answer:\")\n",
    "        labels[:len(question_prefix)] = -100  # -100 is ignored by PyTorch loss\n",
    "        \n",
    "        # Mask out padding tokens in the labels as well\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        # Return a dictionary of tensors for the Trainer\n",
    "        return {\n",
    "            'input_ids':      input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels':         labels,\n",
    "        }\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = QADataset(train_questions, train_answers, tokenizer)\n",
    "val_dataset = QADataset(val_questions, val_answers, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training parameters...\n",
      "Training configuration set up!\n",
      "Will train for 3 epochs\n",
      "Batch size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/insanesac/workdir/envs/vscode/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set up training configuration\n",
    "print(\"Setting up training parameters...\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',           # Where to save the model\n",
    "    num_train_epochs=3,               # How many times to go through all data\n",
    "    per_device_train_batch_size=2,    # How many examples to process at once\n",
    "    per_device_eval_batch_size=2,     # Batch size for validation\n",
    "    warmup_steps=100,                 # Gradual learning rate increase\n",
    "    weight_decay=0.01,                # Regularization to prevent overfitting\n",
    "    logging_dir='./logs',             # Where to save training logs\n",
    "    logging_steps=5,                  # Log progress every 5 steps\n",
    "    evaluation_strategy='epoch',      # Evaluate after each epoch\n",
    "    save_strategy='epoch',            # Save model after each epoch\n",
    "    load_best_model_at_end=True,      # Load the best model when done\n",
    "    metric_for_best_model='eval_loss', # Use validation loss to pick best model\n",
    "    report_to=None,                   # Don't report to wandb/tensorboard\n",
    ")\n",
    "\n",
    "print(\"Training configuration set up!\")\n",
    "print(f\"Will train for {training_args.num_train_epochs} epochs\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "Now we'll create a Trainer instance and start the training process. This might take a few minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating trainer...\n",
      "Starting training... This might take a few minutes.\n",
      "You'll see training progress below:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6594/3899472840.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6d2b8cd1f7468288341132f8d32bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.6355, 'grad_norm': 105.2732925415039, 'learning_rate': 2.5e-06, 'epoch': 0.42}\n",
      "{'loss': 4.993, 'grad_norm': 88.20294189453125, 'learning_rate': 5e-06, 'epoch': 0.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4fcb301db747d09270385ba973100f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.415099143981934, 'eval_runtime': 0.0986, 'eval_samples_per_second': 60.868, 'eval_steps_per_second': 30.434, 'epoch': 1.0}\n",
      "{'loss': 4.4254, 'grad_norm': 61.644622802734375, 'learning_rate': 7.5e-06, 'epoch': 1.25}\n",
      "{'loss': 3.4686, 'grad_norm': 24.273456573486328, 'learning_rate': 1e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122aba9db38b45f6b6d0709a06aa6667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2102718353271484, 'eval_runtime': 0.1067, 'eval_samples_per_second': 56.258, 'eval_steps_per_second': 28.129, 'epoch': 2.0}\n",
      "{'loss': 2.9814, 'grad_norm': 26.527381896972656, 'learning_rate': 1.25e-05, 'epoch': 2.08}\n",
      "{'loss': 2.7553, 'grad_norm': 26.958595275878906, 'learning_rate': 1.5e-05, 'epoch': 2.5}\n",
      "{'loss': 2.7174, 'grad_norm': 26.189979553222656, 'learning_rate': 1.75e-05, 'epoch': 2.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e791ed74a7b4204a038e07c6d0aee8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.97796893119812, 'eval_runtime': 0.0673, 'eval_samples_per_second': 89.201, 'eval_steps_per_second': 44.6, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 8.2583, 'train_samples_per_second': 8.719, 'train_steps_per_second': 4.359, 'train_loss': 3.83468324608273, 'epoch': 3.0}\n",
      "Training completed! üéâ\n",
      "Final training loss: 3.8347\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating trainer...\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"Starting training... This might take a few minutes.\")\n",
    "print(\"You'll see training progress below:\")\n",
    "\n",
    "# Start training\n",
    "training_result = trainer.train()\n",
    "\n",
    "print(\"Training completed! üéâ\")\n",
    "print(f\"Final training loss: {training_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now let's test our fine-tuned model! We'll create a function to generate answers and test it with various questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, max_new_tokens=100, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generate an answer for a given question using our fine-tuned model.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        max_new_tokens: Maximum length of the answer\n",
    "        temperature: Controls randomness (0.1 = focused, 1.0 = creative)\n",
    "    \"\"\"\n",
    "    # Format the question the same way we did during training\n",
    "    prompt = f\"Question: {question} Answer:\"\n",
    "    \n",
    "    # Convert text to model input\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        prompt,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    \n",
    "    # Move to same device as model\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    # Generate answer\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,  # Mask to avoid attending to padding tokens\n",
    "            max_new_tokens=max_new_tokens,  # Maximum number of tokens to generate\n",
    "            min_new_tokens=5,               # Don't allow EOS until at least 5 tokens\n",
    "            do_sample=True,                 # If True, sample from the distribution (more creative); if False, use greedy decoding\n",
    "            temperature=temperature,        # Controls randomness of sampling\n",
    "            num_beams=3,                    # Number of beams for beam search (higher = more thorough search, but slower)\n",
    "            pad_token_id=tokenizer.eos_token_id,  # Token ID used for padding (set to EOS for GPT-2)\n",
    "        )\n",
    "    # Extract only the generated part (remove the input prompt)\n",
    "    generated_tokens = outputs[0][len(input_ids[0]):]\n",
    "    answer = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up the answer\n",
    "    answer = answer.strip()\n",
    "    if answer.startswith(\"Answer:\"):\n",
    "        answer = answer[7:].strip()\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the fine-tuned model with training examples:\n",
      "============================================================\n",
      "‚ùì Question: What are the library hours?\n",
      "ü§ñ Answer: The library is open every Monday through Friday from 9 a.m. to 5 p.m.\n",
      "----------------------------------------\n",
      "‚ùì Question: How can I access campus Wi-Fi?\n",
      "ü§ñ Answer: You can access campus Wi-Fi through the campus Wi-Fi portal.\n",
      "----------------------------------------\n",
      "‚ùì Question: Where is the cafeteria located?\n",
      "ü§ñ Answer: The cafeteria is located in the cafeteria area of the cafeteria area of the cafeteria area of the cafeteria area\n",
      "----------------------------------------\n",
      "\n",
      "Testing with NEW questions (not in training data):\n",
      "============================================================\n",
      "‚ùì Question: What time does the gym open?\n",
      "ü§ñ Answer: 7:00 a.m. to 8:00 p.m. to 9:00 p\n",
      "----------------------------------------\n",
      "‚ùì Question: How do I contact the IT help desk?\n",
      "ü§ñ Answer: You can contact the IT support desk at the IT support desk at the IT support desk at the IT\n",
      "----------------------------------------\n",
      "‚ùì Question: Where can I study late at night?\n",
      "ü§ñ Answer: You can study late at night. You can study late at night. You can study late at night\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test with questions from our training data\n",
    "print(\"Testing the fine-tuned model with training examples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "training_questions = [\n",
    "    \"What are the library hours?\",\n",
    "    \"How can I access campus Wi-Fi?\",\n",
    "    \"Where is the cafeteria located?\"\n",
    "]\n",
    "\n",
    "for question in training_questions:\n",
    "    print(f\"‚ùì Question: {question}\")\n",
    "    answer = generate_answer(question)\n",
    "    print(f\"ü§ñ Answer: {answer}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Test with new questions (not in training data)\n",
    "print(\"\\nTesting with NEW questions (not in training data):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "new_questions = [\n",
    "    \"What time does the gym open?\",\n",
    "    \"How do I contact the IT help desk?\",\n",
    "    \"Where can I study late at night?\"\n",
    "]\n",
    "\n",
    "for question in new_questions:\n",
    "    print(f\"‚ùì Question: {question}\")\n",
    "    answer = generate_answer(question)\n",
    "    print(f\"ü§ñ Answer: {answer}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Debugging model generation...\n",
      "Input prompt: 'Question: What are the library hours? Answer:'\n",
      "Input token IDs shape: torch.Size([1, 10])\n",
      "Input tokens: [24361, 25, 1867, 389, 262, 5888, 2250, 30, 23998, 25]\n",
      "Decoded input: 'Question: What are the library hours? Answer:'\n",
      "Output shape: torch.Size([1, 30])\n",
      "Full output tokens: [24361, 25, 1867, 389, 262, 5888, 2250, 30, 23998, 25, 383, 5888, 318, 1280, 790, 3321, 832, 3217, 422, 860, 257, 13, 76, 13, 284, 642, 279, 13, 76, 13]\n",
      "Full decoded output: 'Question: What are the library hours? Answer: The library is open every Monday through Friday from 9 a.m. to 5 p.m.'\n",
      "Generated tokens only: [383, 5888, 318, 1280, 790, 3321, 832, 3217, 422, 860, 257, 13, 76, 13, 284, 642, 279, 13, 76, 13]\n",
      "Generated answer: ' The library is open every Monday through Friday from 9 a.m. to 5 p.m.'\n",
      "\n",
      "Final answer: ' The library is open every Monday through Friday from 9 a.m. to 5 p.m.'\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Debugging model generation...\")\n",
    "\n",
    "# Test the base model generation first\n",
    "def debug_generation(question):\n",
    "    \"\"\"Debug function to see what's happening during generation\"\"\"\n",
    "    prompt = f\"Question: {question} Answer:\"\n",
    "    print(f\"Input prompt: '{prompt}'\")\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        prompt,\n",
    "        return_tensors='pt',\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    \n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    print(f\"Input token IDs shape: {input_ids.shape}\")\n",
    "    print(f\"Input tokens: {input_ids[0].tolist()}\")\n",
    "    print(f\"Decoded input: '{tokenizer.decode(input_ids[0])}'\")\n",
    "    \n",
    "    # Generate with simpler parameters first\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=20,\n",
    "            min_new_tokens=5,           # don‚Äôt allow EOS until at least 5 tokens\n",
    "            do_sample=False,            # you can mix with or without sampling\n",
    "            num_beams=3,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    \n",
    "    print(f\"Output shape: {outputs.shape}\")\n",
    "    print(f\"Full output tokens: {outputs[0].tolist()}\")\n",
    "    print(f\"Full decoded output: '{tokenizer.decode(outputs[0])}'\")\n",
    "    \n",
    "    # Extract generated part\n",
    "    generated_tokens = outputs[0][len(input_ids[0]):]\n",
    "    print(f\"Generated tokens only: {generated_tokens.tolist()}\")\n",
    "    answer = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    print(f\"Generated answer: '{answer}'\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test with a simple question\n",
    "test_answer = debug_generation(\"What are the library hours?\")\n",
    "print(f\"\\nFinal answer: '{test_answer}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "Congratulations! üéâ You've successfully fine-tuned a language model. Here's what you accomplished:\n",
    "\n",
    "### What You Learned:\n",
    "1. **Data Preparation**: How to format Q&A data for training\n",
    "2. **Model Setup**: Loading and configuring a pre-trained model\n",
    "3. **Fine-tuning**: Training the model on your specific dataset\n",
    "4. **Evaluation**: Testing the model with both seen and unseen questions\n",
    "\n",
    "### How to Improve Your Model:\n",
    "1. **More Data**: Add more question-answer pairs to your dataset\n",
    "2. **Better Prompts**: Experiment with different question formats\n",
    "3. **Hyperparameter Tuning**: Adjust learning rate, batch size, epochs\n",
    "4. **Longer Training**: Train for more epochs (but watch for overfitting)\n",
    "5. **Temperature Tuning**: Adjust temperature in generation for different creativity levels\n",
    "\n",
    "### Understanding the Results:\n",
    "- **Good answers on training questions**: Shows the model learned the training data\n",
    "- **Reasonable answers on new questions**: Shows the model can generalize\n",
    "- **Repetitive or strange answers**: May need more training data or different hyperparameters\n",
    "\n",
    "### Next Steps:\n",
    "- Try fine-tuning on a different domain (customer service, technical docs, etc.)\n",
    "- Experiment with larger models like GPT-3.5 or Llama\n",
    "- Learn about parameter-efficient fine-tuning (LoRA, QLoRA)\n",
    "- Deploy your model as a web API\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "- **Model gives strange answers**: Try adjusting temperature or adding more training data\n",
    "- **Repetitive responses**: Increase `repetition_penalty` parameter\n",
    "- **Too slow**: Reduce batch size or use a smaller model\n",
    "- **Out of memory**: Reduce `max_length` or batch size\n",
    "- **Empty answers**: Check your data formatting and prompt structure\n",
    "\n",
    "Happy fine-tuning! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
