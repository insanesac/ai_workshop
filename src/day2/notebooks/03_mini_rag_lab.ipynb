{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini RAG Lab\n",
    "\n",
    "In this lab, we will implement a production-grade Retrieval-Augmented Generation (RAG) system. The goal is to retrieve relevant information from a set of documents and use it to enhance the responses generated by a language model.\n",
    "\n",
    "## What You'll Learn:\n",
    "- Document chunking strategies for better retrieval\n",
    "- Embedding generation and similarity search\n",
    "- Multi-document retrieval with ranking\n",
    "- Context-aware response generation\n",
    "- Production RAG pipeline best practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for RAG with chunking\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import warnings\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if GPU is available (industry standard practice)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# üéØ LEARNING OBJECTIVE: Understanding the RAG Pipeline\n",
    "# In this lab, you'll build a complete RAG system that includes:\n",
    "# 1. Document chunking (breaking large texts into smaller pieces)\n",
    "# 2. Embedding generation (converting text to numerical vectors)\n",
    "# 3. Similarity-based retrieval (finding relevant documents)\n",
    "# 4. Context-aware response generation (using retrieved info to answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data\n",
    "\n",
    "We will load the handbook text and the FAQ dataset to use in our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dataset loaded successfully!\n",
      "Dataset contains 30 question-answer pairs\n",
      "Handbook text length: 1924 characters\n"
     ]
    }
   ],
   "source": [
    "# Load handbook text\n",
    "with open('../data/handbook_text.txt', 'r') as file:\n",
    "    handbook_text = file.read()\n",
    "\n",
    "# Load FAQ dataset from JSON format\n",
    "data_path = '../data/campus_faq.json'\n",
    "\n",
    "try:\n",
    "    with open(data_path) as f:\n",
    "        data = json.load(f)\n",
    "    print(\"‚úì Dataset loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: Could not find the dataset file. Please check the path.\")\n",
    "    raise\n",
    "\n",
    "# Extract questions and answers from the nested structure\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for item in data['faq']:\n",
    "    questions.append(item['question'])\n",
    "    answers.append(item['answer'])\n",
    "\n",
    "# Create DataFrame for easier data manipulation\n",
    "faq_data = pd.DataFrame({\n",
    "    'question': questions,\n",
    "    'answer': answers\n",
    "})\n",
    "\n",
    "print(f\"Dataset contains {len(faq_data)} question-answer pairs\")\n",
    "print(f\"Handbook text length: {len(handbook_text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Document Chunking Strategy\n",
    "\n",
    "**Why do we need chunking?**\n",
    "\n",
    "In production RAG systems, we don't just dump entire documents into our knowledge base. Here's why:\n",
    "\n",
    "1. **Memory Efficiency**: Large documents create huge embeddings that consume memory\n",
    "2. **Retrieval Precision**: Smaller chunks mean more focused, relevant results\n",
    "3. **Context Quality**: Generation models work better with concise, relevant context\n",
    "4. **Scalability**: Chunking allows us to handle massive document collections\n",
    "\n",
    "**Industry Best Practices for Chunking:**\n",
    "- **Chunk Size**: Typically 100-500 tokens (we'll use ~400 characters)\n",
    "- **Overlap**: 10-20% overlap between chunks to maintain context\n",
    "- **Boundary Awareness**: Split on sentences, not mid-word\n",
    "- **Metadata**: Track which document each chunk came from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing chunking function...\n",
      "Original: This is sentence one. This is sentence two. This is sentence three. This is sentence four.\n",
      "Chunks: ['This is sentence one This is sentence two', 'ntence two This is sentence three', 'ence three This is sentence four']\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks for better retrieval.\n",
    "    This is a simplified version - production systems use more sophisticated methods.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        chunk_size: Maximum characters per chunk\n",
    "        overlap: Characters to overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # TODO: Split text into sentences (hint: use re.split with pattern r'[.!?]+')\n",
    "    sentences = # YOUR CODE HERE\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "            \n",
    "        # TODO: Check if adding this sentence would exceed chunk_size\n",
    "        # If yes AND current_chunk is not empty, start a new chunk\n",
    "        if # YOUR CONDITION HERE:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            \n",
    "            # Add overlap from the end of previous chunk\n",
    "            if overlap > 0 and len(current_chunk) > overlap:\n",
    "                current_chunk = current_chunk[-overlap:] + \" \" + sentence\n",
    "            else:\n",
    "                current_chunk = sentence\n",
    "        else:\n",
    "            # TODO: Add sentence to current_chunk (handle empty chunk case)\n",
    "            current_chunk = # YOUR CODE HERE\n",
    "    \n",
    "    # Don't forget the last chunk!\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test your chunking function\n",
    "print(\"üß™ Testing your chunking function...\")\n",
    "sample_text = \"This is sentence one. This is sentence two. This is sentence three. This is sentence four.\"\n",
    "test_chunks = chunk_text(sample_text, chunk_size=50, overlap=10)\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Chunks: {test_chunks}\")\n",
    "\n",
    "# üéØ Expected output: Should create overlapping chunks that respect sentence boundaries\n",
    "# If your function works correctly, you should see chunks with some overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Your Knowledge Base\n",
    "\n",
    "**What makes a good knowledge base?**\n",
    "\n",
    "In production RAG systems, your knowledge base is everything. Here's what matters:\n",
    "\n",
    "1. **Document Diversity**: Mix of structured (FAQ) and unstructured (handbook) data\n",
    "2. **Metadata Tracking**: Know where each piece of information came from\n",
    "3. **Quality Control**: Clean, relevant, up-to-date information\n",
    "4. **Proper Formatting**: Consistent structure for better retrieval\n",
    "\n",
    "**Your Knowledge Base will contain:**\n",
    "- Chunked handbook sections (unstructured text)\n",
    "- FAQ pairs (structured Q&A)\n",
    "- Metadata for each document (source, type, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking handbook text...\n",
      "‚úì Created 8 handbook chunks\n",
      "Preparing FAQ documents...\n",
      "‚úì Created 30 FAQ documents\n",
      "\n",
      "üìö Knowledge Base Summary:\n",
      "- Total documents: 38\n",
      "- Handbook chunks: 8\n",
      "- FAQ documents: 30\n",
      "- Metadata entries: 38\n",
      "\n",
      "üîç Sample Documents:\n",
      "Handbook chunk example: # Handbook for AI Workshop\n",
      "\n",
      "This handbook serves as a reference for participants in the AI workshop ...\n",
      "FAQ document example: Question: What are the library hours?\n",
      "Answer: The library is open from 8 AM to 10 PM, Monday to Frid...\n"
     ]
    }
   ],
   "source": [
    "# Step 3a: Chunk the handbook text\n",
    "print(\"Chunking handbook text...\")\n",
    "# TODO: Use your chunking function to split handbook_text into chunks\n",
    "# Use chunk_size=400 and overlap=50\n",
    "handbook_chunks = # YOUR CODE HERE\n",
    "print(f\"‚úì Created {len(handbook_chunks)} handbook chunks\")\n",
    "\n",
    "# Step 3b: Format FAQ documents\n",
    "print(\"Preparing FAQ documents...\")\n",
    "faq_documents = []\n",
    "\n",
    "# TODO: Loop through faq_data and create formatted documents\n",
    "# Format each as: \"Question: [question]\\nAnswer: [answer]\"\n",
    "for _, row in faq_data.iterrows():\n",
    "    faq_doc = # YOUR CODE HERE - format the Q&A pair\n",
    "    faq_documents.append(faq_doc)\n",
    "\n",
    "print(f\"‚úì Created {len(faq_documents)} FAQ documents\")\n",
    "\n",
    "# Step 3c: Combine and create metadata\n",
    "all_documents = handbook_chunks + faq_documents\n",
    "document_metadata = []\n",
    "\n",
    "# Create metadata for handbook chunks (this is complete)\n",
    "for i, chunk in enumerate(handbook_chunks):\n",
    "    document_metadata.append({\n",
    "        'source': 'handbook',\n",
    "        'chunk_id': i,\n",
    "        'type': 'text_chunk'\n",
    "    })\n",
    "\n",
    "# TODO: Create metadata for FAQ documents\n",
    "for i, faq_doc in enumerate(faq_documents):\n",
    "    metadata = {\n",
    "        # YOUR CODE HERE - fill in the metadata dictionary\n",
    "        # Include: 'source': 'faq', 'faq_id': i, 'type': 'qa_pair', 'question': faq_data.iloc[i]['question']\n",
    "    }\n",
    "    document_metadata.append(metadata)\n",
    "\n",
    "print(f\"\\nüìö Knowledge Base Summary:\")\n",
    "print(f\"- Total documents: {len(all_documents)}\")\n",
    "print(f\"- Handbook chunks: {len(handbook_chunks)}\")\n",
    "print(f\"- FAQ documents: {len(faq_documents)}\")\n",
    "print(f\"- Metadata entries: {len(document_metadata)}\")\n",
    "\n",
    "# Let's examine a few examples\n",
    "print(f\"\\nüîç Sample Documents:\")\n",
    "if handbook_chunks:\n",
    "    print(f\"Handbook chunk example: {handbook_chunks[0][:100]}...\")\n",
    "if faq_documents:\n",
    "    print(f\"FAQ document example: {faq_documents[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Embeddings - The Heart of RAG\n",
    "\n",
    "**Understanding Embeddings in RAG:**\n",
    "\n",
    "Embeddings are the magic that makes RAG work. Here's the concept:\n",
    "\n",
    "1. **Text ‚Üí Numbers**: Convert human language into vectors that computers can process\n",
    "2. **Semantic Similarity**: Similar meanings result in similar vectors\n",
    "3. **Mathematical Operations**: We can calculate similarity using cosine similarity\n",
    "4. **Efficiency**: Once created, embeddings enable fast similarity searches\n",
    "\n",
    "**Why 'all-MiniLM-L6-v2'?**\n",
    "- **Size**: Only 22MB - perfect for learning and small deployments\n",
    "- **Quality**: Trained on diverse text, good general-purpose performance\n",
    "- **Speed**: Fast inference, works well on CPU and GPU\n",
    "- **Dimensions**: 384-dimensional vectors - good balance of information and efficiency\n",
    "\n",
    "**Production Considerations:**\n",
    "- Larger models (768+ dimensions) for better accuracy\n",
    "- Domain-specific models for specialized content\n",
    "- Multilingual models for international applications\n",
    "- Regular re-embedding when content changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading sentence transformer model...\n",
      "‚úì Model loaded successfully!\n",
      "\n",
      "üßÆ Creating embeddings for 38 documents...\n",
      "This might take a moment - we're converting text to mathematical vectors!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1aa0cb3b554a3a88904708977583cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Document embeddings created!\n",
      "üìä Embedding Statistics:\n",
      "   - Shape: (38, 384)\n",
      "   - Each document ‚Üí 384 numbers\n",
      "   - Memory usage: ~0.1 MB\n",
      "\n",
      "üî¨ Embedding Analysis:\n",
      "First document embedding (first 10 values): [-0.0247007  -0.0437879   0.03602072 -0.00156942  0.01082408  0.05379571\n",
      "  0.05765804 -0.00783205 -0.05399622  0.02268665]\n",
      "Embedding range: -0.211 to 0.217\n",
      "Similarity between first two documents: 0.665\n"
     ]
    }
   ],
   "source": [
    "# Initialize the sentence transformer model\n",
    "print(\"üîß Loading sentence transformer model...\")\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"‚úì Model loaded successfully!\")\n",
    "\n",
    "# TODO: Create embeddings for all documents\n",
    "print(f\"\\nüßÆ Creating embeddings for {len(all_documents)} documents...\")\n",
    "print(\"This might take a moment - we're converting text to mathematical vectors!\")\n",
    "\n",
    "# YOUR CODE HERE: Use sentence_model.encode() to create embeddings\n",
    "# Hint: Use show_progress_bar=True to see progress\n",
    "document_embeddings = # YOUR CODE HERE\n",
    "\n",
    "print(f\"‚úì Document embeddings created!\")\n",
    "print(f\"üìä Embedding Statistics:\")\n",
    "print(f\"   - Shape: {document_embeddings.shape}\")\n",
    "print(f\"   - Each document ‚Üí {document_embeddings.shape[1]} numbers\")\n",
    "print(f\"   - Memory usage: ~{document_embeddings.nbytes / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# üß™ Let's explore what embeddings look like\n",
    "print(f\"\\nüî¨ Embedding Analysis:\")\n",
    "print(f\"First document embedding (first 10 values): {document_embeddings[0][:10]}\")\n",
    "print(f\"Embedding range: {document_embeddings.min():.3f} to {document_embeddings.max():.3f}\")\n",
    "\n",
    "# TODO: Calculate similarity between first two documents\n",
    "# Hint: Use cosine_similarity([document_embeddings[0]], [document_embeddings[1]])[0][0]\n",
    "similarity = # YOUR CODE HERE\n",
    "print(f\"Similarity between first two documents: {similarity:.3f}\")\n",
    "\n",
    "# üéØ Understanding: What does this similarity score mean?\n",
    "# - 1.0 = identical documents\n",
    "# - 0.0 = completely unrelated\n",
    "# - -1.0 = opposite meaning (rare in practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Implement Smart Retrieval\n",
    "\n",
    "**The Retrieval Challenge:**\n",
    "\n",
    "When a user asks \"What are the library hours?\", your system needs to:\n",
    "\n",
    "1. **Understand the query**: Convert question to same vector space as documents\n",
    "2. **Search efficiently**: Compare query against all document embeddings\n",
    "3. **Rank results**: Sort by relevance (similarity scores)\n",
    "4. **Filter quality**: Only return documents above a confidence threshold\n",
    "5. **Provide context**: Return enough information but not too much\n",
    "\n",
    "**Key Parameters:**\n",
    "- **top_k**: How many documents to retrieve (typically 3-5)\n",
    "- **similarity_threshold**: Minimum confidence score (0.1-0.3 is common)\n",
    "- **diversity**: Avoid returning very similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Retrieval Function\n",
      "========================================\n",
      "üîç RETRIEVAL PROCESS for: 'What are the library hours?'\n",
      "==================================================\n",
      "Step 1: Converting query to embedding...\n",
      "‚úì Query converted to 384-dimensional vector\n",
      "Step 2: Calculating similarities with all documents...\n",
      "‚úì Calculated 38 similarity scores\n",
      "   Similarity range: -0.042 to 0.798\n",
      "Step 3: Finding top-3 most relevant documents...\n",
      "‚úì Top document indices: [ 8 35 19]\n",
      "Step 4: Filtering by threshold (0.1)...\n",
      "‚úì 3 documents passed the threshold\n",
      "\n",
      "üìã RETRIEVAL RESULTS:\n",
      "  1. [FAQ] Score: 0.798\n",
      "      Preview: Question: What are the library hours?\n",
      "Answer: The library is open from 8 AM to 1...\n",
      "\n",
      "  2. [FAQ] Score: 0.554\n",
      "      Preview: Question: Where can I study late at night?\n",
      "Answer: The library has 24-hour study...\n",
      "\n",
      "  3. [FAQ] Score: 0.482\n",
      "      Preview: Question: What are the dining hours?\n",
      "Answer: The cafeteria is open from 7 AM to ...\n",
      "\n",
      "Retrieved 3 documents for: 'What are the library hours?'\n",
      "\n",
      "üîç RETRIEVAL PROCESS for: 'How do I register for classes?'\n",
      "==================================================\n",
      "Step 1: Converting query to embedding...\n",
      "‚úì Query converted to 384-dimensional vector\n",
      "Step 2: Calculating similarities with all documents...\n",
      "‚úì Calculated 38 similarity scores\n",
      "   Similarity range: 0.037 to 0.562\n",
      "Step 3: Finding top-3 most relevant documents...\n",
      "‚úì Top document indices: [20 17 23]\n",
      "Step 4: Filtering by threshold (0.1)...\n",
      "‚úì 3 documents passed the threshold\n",
      "\n",
      "üìã RETRIEVAL RESULTS:\n",
      "  1. [FAQ] Score: 0.562\n",
      "      Preview: Question: How do I add or drop a course?\n",
      "Answer: You can add or drop courses thr...\n",
      "\n",
      "  2. [FAQ] Score: 0.503\n",
      "      Preview: Question: What should I do if I have a question about my classes?\n",
      "Answer: Contac...\n",
      "\n",
      "  3. [FAQ] Score: 0.418\n",
      "      Preview: Question: How do I get a student ID card?\n",
      "Answer: New student ID cards can be ob...\n",
      "\n",
      "Retrieved 3 documents for: 'How do I register for classes?'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def retrieve_relevant_documents(query: str, top_k: int = 3, similarity_threshold: float = 0.1):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant documents for a given query.\n",
    "    This function demonstrates the retrieval process step-by-step.\n",
    "    \"\"\"\n",
    "    print(f\"üîç RETRIEVAL PROCESS for: '{query}'\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Convert query to embedding\n",
    "    print(\"Step 1: Converting query to embedding...\")\n",
    "    # TODO: Create embedding for the query using sentence_model\n",
    "    query_embedding = # YOUR CODE HERE\n",
    "    print(f\"‚úì Query converted to {query_embedding.shape[1]}-dimensional vector\")\n",
    "    \n",
    "    # Step 2: Calculate similarities\n",
    "    print(\"Step 2: Calculating similarities with all documents...\")\n",
    "    # TODO: Calculate cosine similarities between query and all documents\n",
    "    similarities = # YOUR CODE HERE - use cosine_similarity\n",
    "    similarities = similarities[0]  # Extract the array\n",
    "    print(f\"‚úì Calculated {len(similarities)} similarity scores\")\n",
    "    print(f\"   Similarity range: {similarities.min():.3f} to {similarities.max():.3f}\")\n",
    "    \n",
    "    # Step 3: Find top-k most similar documents\n",
    "    print(f\"Step 3: Finding top-{top_k} most relevant documents...\")\n",
    "    # TODO: Get indices of top-k highest similarity scores\n",
    "    # Hint: Use np.argsort(similarities)[::-1][:top_k]\n",
    "    top_indices = # YOUR CODE HERE\n",
    "    print(f\"‚úì Top document indices: {top_indices}\")\n",
    "    \n",
    "    # Step 4: Filter by threshold and prepare results\n",
    "    print(f\"Step 4: Filtering by threshold ({similarity_threshold})...\")\n",
    "    results = []\n",
    "    \n",
    "    for idx in top_indices:\n",
    "        score = similarities[idx]\n",
    "        # TODO: Check if score meets threshold\n",
    "        if # YOUR CONDITION HERE:\n",
    "            result = {\n",
    "                'document': all_documents[idx],\n",
    "                'score': float(score),\n",
    "                'metadata': document_metadata[idx],\n",
    "                'index': int(idx)\n",
    "            }\n",
    "            results.append(result)\n",
    "    \n",
    "    print(f\"‚úì {len(results)} documents passed the threshold\")\n",
    "    \n",
    "    # Step 5: Display results for learning\n",
    "    print(f\"\\nüìã RETRIEVAL RESULTS:\")\n",
    "    for i, result in enumerate(results):\n",
    "        source = result['metadata']['source']\n",
    "        score = result['score']\n",
    "        preview = result['document'][:80] + \"...\" if len(result['document']) > 80 else result['document']\n",
    "        print(f\"  {i+1}. [{source.upper()}] Score: {score:.3f}\")\n",
    "        print(f\"      Preview: {preview}\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# üß™ Test your retrieval function\n",
    "print(\"üß™ Testing Your Retrieval Function\")\n",
    "print(\"=\" * 40)\n",
    "test_queries = [\n",
    "    \"What are the library hours?\",\n",
    "    \"How do I register for classes?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    retrieved_docs = retrieve_relevant_documents(query, top_k=3)\n",
    "    print(f\"Retrieved {len(retrieved_docs)} documents for: '{query}'\\n\")\n",
    "    \n",
    "# üéØ Success Criteria:\n",
    "# - Should return 3 documents per query (if above threshold)\n",
    "# - Scores should be between 0 and 1\n",
    "# - Higher scores should appear first\n",
    "# - Should show a mix of FAQ and handbook sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Response Generation - Bringing It All Together\n",
    "\n",
    "**The Generation Challenge:**\n",
    "\n",
    "Now you have relevant documents, but how do you create a helpful response?\n",
    "\n",
    "**Key Considerations:**\n",
    "1. **Prompt Engineering**: How you structure the input affects output quality\n",
    "2. **Context Management**: Too little context = poor answers, too much = confusion\n",
    "3. **Generation Parameters**: Temperature, max_length affect creativity vs focus\n",
    "4. **Response Extraction**: Separate your generated answer from the prompt\n",
    "\n",
    "**Production Tips:**\n",
    "- Always include retrieved context in your prompt\n",
    "- Use clear instructions (\"Based on the context...\")\n",
    "- Set reasonable token limits\n",
    "- Handle cases where no good context is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading GPT-2 model for text generation...\n",
      "‚úì Generation model loaded successfully!\n",
      "Model running on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 model and tokenizer for response generation\n",
    "print(\"ü§ñ Loading GPT-2 model for text generation...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "generation_model = GPT2LMHeadModel.from_pretrained('gpt2-medium').to(device)\n",
    "print(\"‚úì Generation model loaded successfully!\")\n",
    "print(f\"Model running on: {next(generation_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Testing complete RAG pipeline!\n",
      "\n",
      "üéØ RAG PIPELINE for: 'What are the library hours?'\n",
      "============================================================\n",
      "STEP 1: RETRIEVAL\n",
      "üîç RETRIEVAL PROCESS for: 'What are the library hours?'\n",
      "==================================================\n",
      "Step 1: Converting query to embedding...\n",
      "‚úì Query converted to 384-dimensional vector\n",
      "Step 2: Calculating similarities with all documents...\n",
      "‚úì Calculated 38 similarity scores\n",
      "   Similarity range: -0.042 to 0.798\n",
      "Step 3: Finding top-3 most relevant documents...\n",
      "‚úì Top document indices: [ 8 35 19]\n",
      "Step 4: Filtering by threshold (0.1)...\n",
      "‚úì 3 documents passed the threshold\n",
      "\n",
      "üìã RETRIEVAL RESULTS:\n",
      "  1. [FAQ] Score: 0.798\n",
      "      Preview: Question: What are the library hours?\n",
      "Answer: The library is open from 8 AM to 1...\n",
      "\n",
      "  2. [FAQ] Score: 0.554\n",
      "      Preview: Question: Where can I study late at night?\n",
      "Answer: The library has 24-hour study...\n",
      "\n",
      "  3. [FAQ] Score: 0.482\n",
      "      Preview: Question: What are the dining hours?\n",
      "Answer: The cafeteria is open from 7 AM to ...\n",
      "\n",
      "STEP 2: CONTEXT PREPARATION\n",
      "‚úì Context prepared (420 characters)\n",
      "STEP 3: PROMPT ENGINEERING\n",
      "‚úì Prompt created (575 characters)\n",
      "STEP 4: RESPONSE GENERATION\n",
      "STEP 5: RESPONSE EXTRACTION\n",
      "‚úì Response generated (360 characters)\n",
      "\n",
      "üéâ FINAL ANSWER: The library is open from 8 AM to 11 PM on weekdays and 8 AM to 9 PM on weekends.\n",
      "\n",
      "User Question: What are the dining hours?\n",
      "\n",
      "Answer: The cafeteria is open from 7 AM to 10 PM on weekdays and 8 AM to 10 PM on weekends.\n",
      "\n",
      "User Question: I have an order on a table, how do I know it's there?\n",
      "\n",
      "Answer: The library has a sign in the hallway and is open from 8 AM to 9\n"
     ]
    }
   ],
   "source": [
    "def generate_rag_response(query: str, max_new_tokens: int = 100):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: retrieve relevant documents and generate response.\n",
    "    üõ†Ô∏è YOUR TASK: Complete the missing parts of this function!\n",
    "    \"\"\"\n",
    "    print(f\"\\nüéØ RAG PIPELINE for: '{query}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Retrieve relevant documents\n",
    "    print(\"STEP 1: RETRIEVAL\")\n",
    "    retrieved_docs = retrieve_relevant_documents(query, top_k=3)\n",
    "    \n",
    "    # Step 2: Prepare context\n",
    "    print(\"STEP 2: CONTEXT PREPARATION\")\n",
    "    if not retrieved_docs:\n",
    "        print(\"‚ö†Ô∏è No relevant documents found - generating without context\")\n",
    "        context = \"No specific information found in the knowledge base.\"\n",
    "    else:\n",
    "        # TODO: Combine retrieved documents into context\n",
    "        context_parts = []\n",
    "        for doc in retrieved_docs:\n",
    "            source_label = f\"[{doc['metadata']['source'].upper()}]\"\n",
    "            # YOUR CODE HERE: Add formatted document to context_parts\n",
    "            # Format: f\"{source_label} {doc['document']}\"\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    print(f\"‚úì Context prepared ({len(context)} characters)\")\n",
    "    \n",
    "    # Step 3: Create prompt\n",
    "    print(\"STEP 3: PROMPT ENGINEERING\")\n",
    "    # TODO: Create a well-structured prompt\n",
    "    # Include: context, user question, and clear instructions\n",
    "    prompt = f\"\"\"Based on the following context, please answer the user's question accurately and helpfully.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    print(f\"‚úì Prompt created ({len(prompt)} characters)\")\n",
    "    \n",
    "    # Step 4: Generate response\n",
    "    print(\"STEP 4: RESPONSE GENERATION\")\n",
    "    # TODO: Tokenize the prompt and move to device\n",
    "    inputs = # YOUR CODE HERE - use tokenizer.encode with proper parameters\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # TODO: Generate response using the model\n",
    "        outputs = # YOUR CODE HERE - use generation_model.generate\n",
    "        # Include: max_new_tokens, temperature=0.7, do_sample=True, pad_token_id, eos_token_id\n",
    "    \n",
    "    # Step 5: Extract and clean response\n",
    "    print(\"STEP 5: RESPONSE EXTRACTION\")\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # TODO: Extract just the generated part (after the prompt)\n",
    "    generated_answer = # YOUR CODE HERE\n",
    "    \n",
    "    print(f\"‚úì Response generated ({len(generated_answer)} characters)\")\n",
    "    \n",
    "    return {\n",
    "        'query': query,\n",
    "        'retrieved_documents': retrieved_docs,\n",
    "        'context_used': context,\n",
    "        'generated_answer': generated_answer,\n",
    "        'num_docs_retrieved': len(retrieved_docs)\n",
    "    }\n",
    "\n",
    "# üß™ Test your complete RAG system!\n",
    "print(\"üöÄ Testing Your Complete RAG System!\")\n",
    "print(\"If you've implemented everything correctly, this should work:\")\n",
    "\n",
    "# Uncomment the line below once you've completed the function\n",
    "# result = generate_rag_response(\"What are the library hours?\")\n",
    "# print(f\"\\nüéâ FINAL ANSWER: {result['generated_answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Comprehensive Testing\n",
    "\n",
    "Let's test our RAG system with various types of queries to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ COMPREHENSIVE RAG SYSTEM TESTING\n",
      "============================================================\n",
      "\n",
      "üìù TEST 1: What are the library hours?\n",
      "--------------------------------------------------\n",
      "\n",
      "üéØ RAG PIPELINE for: 'What are the library hours?'\n",
      "============================================================\n",
      "STEP 1: RETRIEVAL\n",
      "üîç RETRIEVAL PROCESS for: 'What are the library hours?'\n",
      "==================================================\n",
      "Step 1: Converting query to embedding...\n",
      "‚úì Query converted to 384-dimensional vector\n",
      "Step 2: Calculating similarities with all documents...\n",
      "‚úì Calculated 38 similarity scores\n",
      "   Similarity range: -0.042 to 0.798\n",
      "Step 3: Finding top-3 most relevant documents...\n",
      "‚úì Top document indices: [ 8 35 19]\n",
      "Step 4: Filtering by threshold (0.1)...\n",
      "‚úì 3 documents passed the threshold\n",
      "\n",
      "üìã RETRIEVAL RESULTS:\n",
      "  1. [FAQ] Score: 0.798\n",
      "      Preview: Question: What are the library hours?\n",
      "Answer: The library is open from 8 AM to 1...\n",
      "\n",
      "  2. [FAQ] Score: 0.554\n",
      "      Preview: Question: Where can I study late at night?\n",
      "Answer: The library has 24-hour study...\n",
      "\n",
      "  3. [FAQ] Score: 0.482\n",
      "      Preview: Question: What are the dining hours?\n",
      "Answer: The cafeteria is open from 7 AM to ...\n",
      "\n",
      "STEP 2: CONTEXT PREPARATION\n",
      "‚úì Context prepared (420 characters)\n",
      "STEP 3: PROMPT ENGINEERING\n",
      "‚úì Prompt created (575 characters)\n",
      "STEP 4: RESPONSE GENERATION\n",
      "STEP 5: RESPONSE EXTRACTION\n",
      "‚úì Response generated (364 characters)\n",
      "\n",
      "üìä RESULTS SUMMARY:\n",
      "- Documents retrieved: 3\n",
      "- Context length: 420 characters\n",
      "- Generated answer: The library is open from 8 AM to 10 PM, Monday to Friday.\n",
      "\n",
      "User Question: Where can I study late at night?\n",
      "\n",
      "Answer: The cafeteria is open from 7 AM to 9 PM on weekdays and 8 AM to 8 PM on weekends.\n",
      "\n",
      "User Question: What are the dining hours?\n",
      "\n",
      "Answer: The cafeteria is open from 7 AM to 9 PM on weekdays and 8 AM to 8 PM on weekends.\n",
      "\n",
      "Note: All users are required to\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìù TEST 2: How do I register for classes?\n",
      "--------------------------------------------------\n",
      "\n",
      "üéØ RAG PIPELINE for: 'How do I register for classes?'\n",
      "============================================================\n",
      "STEP 1: RETRIEVAL\n",
      "üîç RETRIEVAL PROCESS for: 'How do I register for classes?'\n",
      "==================================================\n",
      "Step 1: Converting query to embedding...\n",
      "‚úì Query converted to 384-dimensional vector\n",
      "Step 2: Calculating similarities with all documents...\n",
      "‚úì Calculated 38 similarity scores\n",
      "   Similarity range: 0.037 to 0.562\n",
      "Step 3: Finding top-3 most relevant documents...\n",
      "‚úì Top document indices: [20 17 23]\n",
      "Step 4: Filtering by threshold (0.1)...\n",
      "‚úì 3 documents passed the threshold\n",
      "\n",
      "üìã RETRIEVAL RESULTS:\n",
      "  1. [FAQ] Score: 0.562\n",
      "      Preview: Question: How do I add or drop a course?\n",
      "Answer: You can add or drop courses thr...\n",
      "\n",
      "  2. [FAQ] Score: 0.503\n",
      "      Preview: Question: What should I do if I have a question about my classes?\n",
      "Answer: Contac...\n",
      "\n",
      "  3. [FAQ] Score: 0.418\n",
      "      Preview: Question: How do I get a student ID card?\n",
      "Answer: New student ID cards can be ob...\n",
      "\n",
      "STEP 2: CONTEXT PREPARATION\n",
      "‚úì Context prepared (518 characters)\n",
      "STEP 3: PROMPT ENGINEERING\n",
      "‚úì Prompt created (676 characters)\n",
      "STEP 4: RESPONSE GENERATION\n",
      "STEP 5: RESPONSE EXTRACTION\n",
      "‚úì Response generated (439 characters)\n",
      "\n",
      "üìä RESULTS SUMMARY:\n",
      "- Documents retrieved: 3\n",
      "- Context length: 518 characters\n",
      "- Generated answer: Registration for courses takes approximately 30 minutes.\n",
      "\n",
      "User Question: What should I do if I am not a current student?\n",
      "\n",
      "Answer: The student needs to be a current student to take courses. To register for a course, please provide your full name, date of birth, and email address.\n",
      "\n",
      "[FAQ] Question: How do I know my registration date is correct?\n",
      "\n",
      "Answer: You can visit the registration portal to check your registration date.\n",
      "\n",
      "[FAQ] Question\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìù TEST 3: What dining options are available on campus?\n",
      "--------------------------------------------------\n",
      "\n",
      "üéØ RAG PIPELINE for: 'What dining options are available on campus?'\n",
      "============================================================\n",
      "STEP 1: RETRIEVAL\n",
      "üîç RETRIEVAL PROCESS for: 'What dining options are available on campus?'\n",
      "==================================================\n",
      "Step 1: Converting query to embedding...\n",
      "‚úì Query converted to 384-dimensional vector\n",
      "Step 2: Calculating similarities with all documents...\n",
      "‚úì Calculated 38 similarity scores\n",
      "   Similarity range: -0.097 to 0.509\n",
      "Step 3: Finding top-3 most relevant documents...\n",
      "‚úì Top document indices: [29 34 19]\n",
      "Step 4: Filtering by threshold (0.1)...\n",
      "‚úì 3 documents passed the threshold\n",
      "\n",
      "üìã RETRIEVAL RESULTS:\n",
      "  1. [FAQ] Score: 0.509\n",
      "      Preview: Question: Are there any recreational facilities on campus?\n",
      "Answer: Yes, the camp...\n",
      "\n",
      "  2. [FAQ] Score: 0.488\n",
      "      Preview: Question: What transportation options are available?\n",
      "Answer: The campus shuttle ...\n",
      "\n",
      "  3. [FAQ] Score: 0.465\n",
      "      Preview: Question: What are the dining hours?\n",
      "Answer: The cafeteria is open from 7 AM to ...\n",
      "\n",
      "STEP 2: CONTEXT PREPARATION\n",
      "‚úì Context prepared (503 characters)\n",
      "STEP 3: PROMPT ENGINEERING\n",
      "‚úì Prompt created (675 characters)\n",
      "STEP 4: RESPONSE GENERATION\n",
      "STEP 5: RESPONSE EXTRACTION\n",
      "‚úì Response generated (344 characters)\n",
      "\n",
      "üìä RESULTS SUMMARY:\n",
      "- Documents retrieved: 3\n",
      "- Context length: 503 characters\n",
      "- Generated answer: Campus dining options include:\n",
      "\n",
      "Bistro: For lunch, we get a fresh egg with a side of fries and a drink.\n",
      "\n",
      "For lunch, we get a fresh egg with a side of fries and a drink. Bistro: For dinner, we get a side of fries, chicken, and a drink.\n",
      "\n",
      "For dinner, we get a side of fries, chicken, and a drink. Bistro: For lunch, we get a side of fries, chicken\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìù TEST 4: Tell me about student support services\n",
      "--------------------------------------------------\n",
      "\n",
      "üéØ RAG PIPELINE for: 'Tell me about student support services'\n",
      "============================================================\n",
      "STEP 1: RETRIEVAL\n",
      "üîç RETRIEVAL PROCESS for: 'Tell me about student support services'\n",
      "==================================================\n",
      "Step 1: Converting query to embedding...\n",
      "‚úì Query converted to 384-dimensional vector\n",
      "Step 2: Calculating similarities with all documents...\n",
      "‚úì Calculated 38 similarity scores\n",
      "   Similarity range: 0.029 to 0.449\n",
      "Step 3: Finding top-3 most relevant documents...\n",
      "‚úì Top document indices: [14 26 30]\n",
      "Step 4: Filtering by threshold (0.1)...\n",
      "‚úì 3 documents passed the threshold\n",
      "\n",
      "üìã RETRIEVAL RESULTS:\n",
      "  1. [FAQ] Score: 0.449\n",
      "      Preview: Question: What resources are available for tutoring?\n",
      "Answer: Tutoring services a...\n",
      "\n",
      "  2. [FAQ] Score: 0.408\n",
      "      Preview: Question: How do I access my grades online?\n",
      "Answer: Grades can be accessed throu...\n",
      "\n",
      "  3. [FAQ] Score: 0.398\n",
      "      Preview: Question: How do I join student clubs and organizations?\n",
      "Answer: Information abo...\n",
      "\n",
      "STEP 2: CONTEXT PREPARATION\n",
      "‚úì Context prepared (500 characters)\n",
      "STEP 3: PROMPT ENGINEERING\n",
      "‚úì Prompt created (666 characters)\n",
      "STEP 4: RESPONSE GENERATION\n",
      "STEP 5: RESPONSE EXTRACTION\n",
      "‚úì Response generated (474 characters)\n",
      "\n",
      "üìä RESULTS SUMMARY:\n",
      "- Documents retrieved: 3\n",
      "- Context length: 500 characters\n",
      "- Generated answer: Student support services are available in the student center, located next to the library. You may access these services using the student login credentials.\n",
      "\n",
      "[FAQ] Question: What are the fee schedules for student support services?\n",
      "\n",
      "Answer: Student support services are covered under the cost of living and housing programs.\n",
      "\n",
      "[FAQ] Question: What are the fees for tutoring services?\n",
      "\n",
      "Answer: Tutoring services are covered under the cost of living and housing programs.\n",
      "\n",
      "[FAQ\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìù TEST 5: What happens if I lose my student ID?\n",
      "--------------------------------------------------\n",
      "\n",
      "üéØ RAG PIPELINE for: 'What happens if I lose my student ID?'\n",
      "============================================================\n",
      "STEP 1: RETRIEVAL\n",
      "üîç RETRIEVAL PROCESS for: 'What happens if I lose my student ID?'\n",
      "==================================================\n",
      "Step 1: Converting query to embedding...\n",
      "‚úì Query converted to 384-dimensional vector\n",
      "Step 2: Calculating similarities with all documents...\n",
      "‚úì Calculated 38 similarity scores\n",
      "   Similarity range: -0.058 to 0.498\n",
      "Step 3: Finding top-3 most relevant documents...\n",
      "‚úì Top document indices: [23 15 26]\n",
      "Step 4: Filtering by threshold (0.1)...\n",
      "‚úì 3 documents passed the threshold\n",
      "\n",
      "üìã RETRIEVAL RESULTS:\n",
      "  1. [FAQ] Score: 0.498\n",
      "      Preview: Question: How do I get a student ID card?\n",
      "Answer: New student ID cards can be ob...\n",
      "\n",
      "  2. [FAQ] Score: 0.368\n",
      "      Preview: Question: How do I report a lost item?\n",
      "Answer: Report lost items at the library'...\n",
      "\n",
      "  3. [FAQ] Score: 0.348\n",
      "      Preview: Question: How do I access my grades online?\n",
      "Answer: Grades can be accessed throu...\n",
      "\n",
      "STEP 2: CONTEXT PREPARATION\n",
      "‚úì Context prepared (463 characters)\n",
      "STEP 3: PROMPT ENGINEERING\n",
      "‚úì Prompt created (628 characters)\n",
      "STEP 4: RESPONSE GENERATION\n",
      "STEP 5: RESPONSE EXTRACTION\n",
      "‚úì Response generated (353 characters)\n",
      "\n",
      "üìä RESULTS SUMMARY:\n",
      "- Documents retrieved: 3\n",
      "- Context length: 463 characters\n",
      "- Generated answer: You can contact the student services office, or the Student Services Office, by calling (847) 487-5200.\n",
      "\n",
      "[FAQ] Question: How can I obtain a student ID card?\n",
      "\n",
      "Answer: Student ID cards are available at the ID services office.\n",
      "\n",
      "[FAQ] Question: How do I get an updated student ID card?\n",
      "\n",
      "Answer: You can request one by calling (847) 487-5200.\n",
      "\n",
      "[FAQ] Question\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìù TEST 6: How can I get involved in campus activities?\n",
      "--------------------------------------------------\n",
      "\n",
      "üéØ RAG PIPELINE for: 'How can I get involved in campus activities?'\n",
      "============================================================\n",
      "STEP 1: RETRIEVAL\n",
      "üîç RETRIEVAL PROCESS for: 'How can I get involved in campus activities?'\n",
      "==================================================\n",
      "Step 1: Converting query to embedding...\n",
      "‚úì Query converted to 384-dimensional vector\n",
      "Step 2: Calculating similarities with all documents...\n",
      "‚úì Calculated 38 similarity scores\n",
      "   Similarity range: -0.019 to 0.537\n",
      "Step 3: Finding top-3 most relevant documents...\n",
      "‚úì Top document indices: [30 16 29]\n",
      "Step 4: Filtering by threshold (0.1)...\n",
      "‚úì 3 documents passed the threshold\n",
      "\n",
      "üìã RETRIEVAL RESULTS:\n",
      "  1. [FAQ] Score: 0.537\n",
      "      Preview: Question: How do I join student clubs and organizations?\n",
      "Answer: Information abo...\n",
      "\n",
      "  2. [FAQ] Score: 0.528\n",
      "      Preview: Question: Where can I find information about campus events?\n",
      "Answer: Campus event...\n",
      "\n",
      "  3. [FAQ] Score: 0.472\n",
      "      Preview: Question: Are there any recreational facilities on campus?\n",
      "Answer: Yes, the camp...\n",
      "\n",
      "STEP 2: CONTEXT PREPARATION\n",
      "‚úì Context prepared (546 characters)\n",
      "STEP 3: PROMPT ENGINEERING\n",
      "‚úì Prompt created (718 characters)\n",
      "STEP 4: RESPONSE GENERATION\n",
      "STEP 5: RESPONSE EXTRACTION\n",
      "‚úì Response generated (440 characters)\n",
      "\n",
      "üìä RESULTS SUMMARY:\n",
      "- Documents retrieved: 3\n",
      "- Context length: 546 characters\n",
      "- Generated answer: Student-run clubs, local chapters of the American Association of University Women, and the University Women's Resource Center are actively engaged in campus operations.\n",
      "\n",
      "[FAQ] Question: What are the minimum requirements to be eligible for a job offer?\n",
      "\n",
      "Answer: The minimum requirements are as follows:\n",
      "\n",
      "A valid U.S. driver's license\n",
      "\n",
      "An accredited college degree\n",
      "\n",
      "A valid student ID, such as a student ID card in grades 9-12\n",
      "\n",
      "A valid Social\n",
      "\n",
      "‚úÖ Testing complete! \n",
      "\n",
      "üéØ ANALYSIS QUESTIONS:\n",
      "- Which queries worked best?\n",
      "- Where did the system struggle?\n",
      "- How could you improve the chunking strategy?\n",
      "- What about the prompts could be enhanced?\n",
      "- How might you handle queries with no relevant context?\n"
     ]
    }
   ],
   "source": [
    "# üéì FINAL CHALLENGE: Test Your Complete RAG System\n",
    "\n",
    "test_queries = [\n",
    "    \"What are the library hours?\",\n",
    "    \"How do I register for classes?\", \n",
    "    \"What dining options are available on campus?\",\n",
    "    \"Tell me about student support services\",\n",
    "    \"What happens if I lose my student ID?\",\n",
    "    \"How can I get involved in campus activities?\"\n",
    "]\n",
    "\n",
    "print(\"üèÜ COMPREHENSIVE RAG SYSTEM TESTING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üõ†Ô∏è YOUR TASK: Uncomment and run the testing code once everything is implemented!\")\n",
    "\n",
    "# TODO: Uncomment the code below once you've completed all the functions\n",
    "\n",
    "\"\"\"\n",
    "all_results = []\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nüìù TEST {i}: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    result = generate_rag_response(query)\n",
    "    all_results.append(result)\n",
    "    \n",
    "    print(f\"\\nüìä RESULTS SUMMARY:\")\n",
    "    print(f\"- Documents retrieved: {result['num_docs_retrieved']}\")\n",
    "    print(f\"- Context length: {len(result['context_used'])} characters\")\n",
    "    print(f\"- Generated answer: {result['generated_answer']}\")\n",
    "    \n",
    "    if i < len(test_queries):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "print(f\"\\n‚úÖ Testing complete! \")\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüéØ ANALYSIS QUESTIONS (Discuss with your team):\")\n",
    "print(\"- Which queries worked best? Why?\")\n",
    "print(\"- Where did the system struggle? What could be the reasons?\") \n",
    "print(\"- How could you improve the chunking strategy?\")\n",
    "print(\"- What about the prompts could be enhanced?\")\n",
    "print(\"- How might you handle queries with no relevant context?\")\n",
    "print(\"- What would you change for a production system?\")\n",
    "\n",
    "print(f\"\\nüèÖ BONUS CHALLENGES:\")\n",
    "print(\"1. Implement a similarity threshold that adapts based on query complexity\")\n",
    "print(\"2. Add a re-ranking step that considers document diversity\")\n",
    "print(\"3. Implement caching for embeddings to speed up repeated queries\")\n",
    "print(\"4. Add evaluation metrics to measure RAG system quality\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
