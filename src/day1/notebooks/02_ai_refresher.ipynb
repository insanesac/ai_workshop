{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1: AI Refresher - Understanding the Fundamentals\n",
    "\n",
    "Welcome to the AI Refresher! This notebook is designed to help you understand the core concepts of AI/ML through hands-on exploration and research.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will:\n",
    "- Understand the ML pipeline: data ‚Üí train ‚Üí validate ‚Üí infer\n",
    "- Implement a basic neural network from scratch\n",
    "- Explore forward and backward propagation\n",
    "- Understand loss functions and optimization\n",
    "- Connect theory to real-world AI applications\n",
    "\n",
    "## üìö Research Tasks\n",
    "Throughout this notebook, you'll see **üîç RESEARCH** sections. These require you to:\n",
    "1. Research the concept online\n",
    "2. Discuss with your pair partner\n",
    "3. Implement the missing code\n",
    "4. Test your understanding\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Machine Learning Pipeline\n",
    "\n",
    "Every ML project follows the same basic pipeline:\n",
    "**Data ‚Üí Train ‚Üí Validate ‚Üí Infer**\n",
    "\n",
    "Let's explore each step with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Generation and Exploration\n",
    "\n",
    "üîç **RESEARCH TASK 1**: \n",
    "- What is the difference between supervised and unsupervised learning?\n",
    "- What type of problem are we creating below?\n",
    "- Research `make_classification` function parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "# TODO: Fill in the parameters for make_classification\n",
    "# Hint: We want 1000 samples, 2 features, 2 classes, and some noise\n",
    "X, y = make_classification(\n",
    "    n_samples=____,     # Number of samples\n",
    "    n_features=____,    # Number of features  \n",
    "    n_classes=____,     # Number of classes\n",
    "    n_redundant=0,      # No redundant features\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Unique labels: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# TODO: Create a scatter plot of the data\n",
    "# Hint: Use different colors for different classes\n",
    "# Use plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "plt.scatter(____)\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Synthetic Binary Classification Dataset')\n",
    "plt.colorbar(label='Class')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# TODO: Calculate and print basic statistics\n",
    "print(f\"Class distribution: {____}\")  # Hint: Use np.bincount(y)\n",
    "print(f\"Feature 1 range: [{____:.2f}, {____:.2f}]\")  # Hint: Use X[:, 0].min() and max()\n",
    "print(f\"Feature 2 range: [{____:.2f}, {____:.2f}]\")  # Hint: Use X[:, 1].min() and max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Splitting\n",
    "\n",
    "üîç **RESEARCH TASK 2**:\n",
    "- Why do we split data into train/validation/test sets?\n",
    "- What is the typical ratio for each split?\n",
    "- What is the difference between validation and test sets?\n",
    "- Research the concept of \"data leakage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split the data into training and testing sets\n",
    "# Hint: Use train_test_split with test_size=0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    ____,  # Features\n",
    "    ____,  # Labels  \n",
    "    test_size=____,  # What percentage for testing?\n",
    "    random_state=42,\n",
    "    stratify=____    # Should we maintain class balance? Use y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "print(f\"Training labels distribution: {____}\")  # Use np.bincount\n",
    "print(f\"Testing labels distribution: {____}\")   # Use np.bincount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Networks from Scratch\n",
    "\n",
    "Let's build a simple neural network to understand the fundamentals.\n",
    "\n",
    "üîç **RESEARCH TASK 3**:\n",
    "- What is a perceptron?\n",
    "- What is the difference between a perceptron and a neural network?\n",
    "- Research the sigmoid activation function\n",
    "- What is the purpose of activation functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize a simple 2-layer neural network\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_size: Number of neurons in hidden layer\n",
    "            output_size: Number of output neurons\n",
    "        \"\"\"\n",
    "        # TODO: Initialize weights and biases\n",
    "        # Hint: Use np.random.randn() for random initialization\n",
    "        # Scale by 0.01 to keep values small\n",
    "        \n",
    "        # Weights from input to hidden layer\n",
    "        self.W1 = np.random.randn(____, ____) * 0.01  # Shape: (input_size, hidden_size)\n",
    "        self.b1 = np.zeros((1, ____))                 # Shape: (1, hidden_size)\n",
    "        \n",
    "        # Weights from hidden to output layer  \n",
    "        self.W2 = np.random.randn(____, ____) * 0.01  # Shape: (hidden_size, output_size)\n",
    "        self.b2 = np.zeros((1, ____))                 # Shape: (1, output_size)\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Implement sigmoid activation function\n",
    "        Formula: 1 / (1 + exp(-x))\n",
    "        \n",
    "        Hint: Use np.exp() and be careful of numerical overflow\n",
    "        You might want to clip x to prevent overflow\n",
    "        \"\"\"\n",
    "        # Clip x to prevent overflow\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return ____\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Implement derivative of sigmoid\n",
    "        Formula: sigmoid(x) * (1 - sigmoid(x))\n",
    "        \"\"\"\n",
    "        sig = self.sigmoid(x)\n",
    "        return ____\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        TODO: Implement forward propagation\n",
    "        \n",
    "        Steps:\n",
    "        1. Calculate z1 = X @ W1 + b1\n",
    "        2. Calculate a1 = sigmoid(z1)\n",
    "        3. Calculate z2 = a1 @ W2 + b2  \n",
    "        4. Calculate a2 = sigmoid(z2)\n",
    "        \n",
    "        Store intermediate values for backpropagation\n",
    "        \"\"\"\n",
    "        # Input to hidden layer\n",
    "        self.z1 = ____  # Linear combination\n",
    "        self.a1 = ____  # Activation\n",
    "        \n",
    "        # Hidden to output layer\n",
    "        self.z2 = ____  # Linear combination\n",
    "        self.a2 = ____  # Final output\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        TODO: Implement binary cross-entropy loss\n",
    "        Formula: -[y*log(y_pred) + (1-y)*log(1-y_pred)]\n",
    "        \n",
    "        Hint: Add small epsilon to prevent log(0)\n",
    "        \"\"\"\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        loss = ____  # Implement the formula above\n",
    "        return np.mean(loss)\n",
    "\n",
    "# Test the network initialization\n",
    "network = SimpleNeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
    "print(f\"W1 shape: {network.W1.shape}\")\n",
    "print(f\"W2 shape: {network.W2.shape}\")\n",
    "print(\"Network initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Forward Propagation\n",
    "\n",
    "üîç **RESEARCH TASK 4**:\n",
    "- What happens in forward propagation?\n",
    "- Why do we need to store intermediate values?\n",
    "- What should the output range be for binary classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward propagation with a few samples\n",
    "test_input = X_train[:5]  # Take first 5 samples\n",
    "test_labels = y_train[:5].reshape(-1, 1)  # Reshape for consistency\n",
    "\n",
    "# TODO: Run forward propagation\n",
    "predictions = network.forward(____)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"Predictions: {predictions.flatten()}\")\n",
    "print(f\"True labels: {test_labels.flatten()}\")\n",
    "\n",
    "# TODO: Calculate loss\n",
    "loss = network.compute_loss(____, ____)\n",
    "print(f\"Initial loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Backpropagation and Training\n",
    "\n",
    "üîç **RESEARCH TASK 5**:\n",
    "- What is backpropagation?\n",
    "- How does gradient descent work?\n",
    "- What is a learning rate and how do you choose it?\n",
    "- Research the chain rule in calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add backpropagation to our network\n",
    "def backward(self, X, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    TODO: Implement backpropagation\n",
    "    \n",
    "    WHAT IS BACKPROPAGATION?\n",
    "    Backpropagation is like \"learning from mistakes\" - it calculates how much each weight\n",
    "    contributed to the error, then adjusts weights to reduce that error.\n",
    "    \n",
    "    This is challenging! You need to:\n",
    "    1. Calculate gradients for output layer\n",
    "    2. Calculate gradients for hidden layer\n",
    "    3. Update weights and biases\n",
    "    \n",
    "    Research the math behind backpropagation!\n",
    "    \"\"\"\n",
    "    m = X.shape[0]  # Number of samples\n",
    "    \n",
    "    # STEP 1: Output layer gradients\n",
    "    # How wrong were our predictions? (This is our starting error)\n",
    "    # For binary cross-entropy loss: dL/dz2 = (y_pred - y_true) / m\n",
    "    dz2 = y_pred - y_true  # Derivative of loss w.r.t. z2\n",
    "    # How much did each weight W2 contribute to the error?\n",
    "    # Chain rule: dL/dW2 = dL/dz2 √ó dz2/dW2\n",
    "    # Since z2 = a1 @ W2 + b2, changing W2 by ŒîW2 changes z2 by a1 @ ŒîW2\n",
    "    # So the gradient is: a1.T @ dz2\n",
    "    dW2 = ____ # TODO: Calculate gradient for W2 (use self.a1)\n",
    "    # For bias: dL/db2 = dL/dz2 * dz2/db2 = dL/dz2 * 1\n",
    "    # Since z2 = a1 @ W2 + b2, changing b2 by Œîb2 changes z2 by Œîb2\n",
    "    # So the gradient is just dz2, but we sum over the batch\n",
    "    # The bias b2 contributed to ALL of the errors in dz2\n",
    "    # So the total effect of b2 is the SUM of all errors:\n",
    "    db2 = ____  # TODO: Calculate gradient for b2\n",
    "    \n",
    "    # STEP 2: Hidden layer gradients  \n",
    "    # Backpropagate error from output to hidden layer\n",
    "    # Chain rule: dL/da1 = dL/dz2 * dz2/da1\n",
    "    # Since z2 = a1 @ W2 + b2, changing a1 by Œîa1 changes z2 by Œîa1 @ W2\n",
    "    # So: da1 = dz2 @ W2.T\n",
    "    da1 = ____  # TODO: Backpropagate error to hidden layer\n",
    "    # We have error w.r.t. a1, but we need error w.r.t. z1\n",
    "    # Since a1 = sigmoid(z1), we use: dz1 = da1 * sigmoid'(z1)\n",
    "    dz1 = ____  # TODO: Apply derivative of activation function\n",
    "    # Since z1 = X @ W1 + b1:\n",
    "    dW1 = ____  # TODO: Calculate gradient for W1\n",
    "    db1 = ____  # TODO: Calculate gradient for b1\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_weights(self, dW1, db1, dW2, db2, learning_rate):\n",
    "    \"\"\"\n",
    "    TODO: Update weights using gradient descent\n",
    "    Formula: weight = weight - learning_rate * gradient\n",
    "    \"\"\"\n",
    "    self.W1 -= ____\n",
    "    self.b1 -= ____\n",
    "    self.W2 -= ____\n",
    "    self.b2 -= ____\n",
    "\n",
    "# Add methods to the class\n",
    "SimpleNeuralNetwork.backward = backward\n",
    "SimpleNeuralNetwork.update_weights = update_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "üîç **RESEARCH TASK 6**:\n",
    "- What is an epoch?\n",
    "- What is batch training vs. online training?\n",
    "- How do you know when to stop training?\n",
    "- Research overfitting and underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "X_train_norm = (X_train - X_train.mean(axis=0)) / X_train.std(axis=0)  # Normalize features\n",
    "X_test_norm = (X_test - X_train.mean(axis=0)) / X_train.std(axis=0)    # Use training stats\n",
    "y_train_reshaped = y_train.reshape(-1, 1)\n",
    "y_test_reshaped = y_test.reshape(-1, 1)\n",
    "\n",
    "# Training parameters\n",
    "epochs = ____      # TODO: How many epochs? Try 1000\n",
    "learning_rate = ____  # TODO: What learning rate? Try 0.1\n",
    "\n",
    "# Initialize network\n",
    "network = SimpleNeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
    "\n",
    "# Storage for plotting\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# TODO: Implement training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward propagation\n",
    "    train_pred = network.forward(____)\n",
    "    \n",
    "    # Calculate loss\n",
    "    train_loss = network.compute_loss(____, ____)\n",
    "    \n",
    "    # Backpropagation\n",
    "    dW1, db1, dW2, db2 = network.backward(____, ____, ____)\n",
    "    \n",
    "    # Update weights\n",
    "    network.update_weights(____, ____, ____, ____, ____)\n",
    "    \n",
    "    # Evaluate on test set (for monitoring)\n",
    "    test_pred = network.forward(____)\n",
    "    test_loss = network.compute_loss(____, ____)\n",
    "    \n",
    "    # Store losses\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Test Loss = {test_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Training Results\n",
    "\n",
    "üîç **RESEARCH TASK 7**:\n",
    "- How do you interpret loss curves?\n",
    "- What does it mean if train loss decreases but test loss increases?\n",
    "- What metrics can we use to evaluate binary classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1, 2, 1)\n",
    "# TODO: Plot training and test losses\n",
    "plt.plot(___, label='Training Loss')\n",
    "plt.plot(____, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Decision boundary visualization\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "# Create a mesh to plot decision boundary\n",
    "h = 0.02\n",
    "x_min, x_max = X_train_norm[:, 0].min() - 1, X_train_norm[:, 0].max() + 1\n",
    "y_min, y_max = X_train_norm[:, 1].min() - 1, X_train_norm[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# TODO: Make predictions on the mesh\n",
    "mesh_points = np.column_stack([xx.ravel(), yy.ravel()])\n",
    "Z = network.forward(____).reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')\n",
    "plt.colorbar(label='Prediction')\n",
    "\n",
    "# Plot training data\n",
    "scatter = plt.scatter(X_train_norm[:, 0], X_train_norm[:, 1], c=y_train, cmap='RdYlBu', edgecolors='black')\n",
    "plt.xlabel('Feature 1 (normalized)')\n",
    "plt.ylabel('Feature 2 (normalized)')\n",
    "plt.title('Decision Boundary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    TODO: Calculate classification accuracy\n",
    "    Hint: Convert probabilities to binary predictions using threshold 0.5\n",
    "    \"\"\"\n",
    "    binary_pred = (y_pred > ____).astype(int)\n",
    "    accuracy = ____  # Compare with y_true and calculate mean\n",
    "    return accuracy\n",
    "\n",
    "# Get final predictions\n",
    "train_pred_final = network.forward(X_train_norm)\n",
    "test_pred_final = network.forward(X_test_norm)\n",
    "\n",
    "# TODO: Calculate accuracies\n",
    "train_accuracy = calculate_accuracy(____, ____)\n",
    "test_accuracy = calculate_accuracy(____, ____)\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final Test Loss: {test_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Real-World AI Applications\n",
    "\n",
    "üîç **RESEARCH TASK 8**:\n",
    "Research and discuss with your partner:\n",
    "- How are the concepts we just learned used in:\n",
    "  - **Chatbots**: How do language models use neural networks?\n",
    "  - **Recommendation engines**: How does Netflix recommend movies?\n",
    "  - **Image detectors**: How does your phone recognize faces?\n",
    "- What are the key differences between the simple network we built and modern deep learning models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion Questions (Work with your partner)\n",
    "\n",
    "**TODO: Discuss and write your answers below:**\n",
    "\n",
    "1. **Chatbots (like ChatGPT)**:\n",
    "   - How might the forward propagation we implemented relate to text generation?\n",
    "   - *Your answer: ____*\n",
    "\n",
    "2. **Recommendation Systems**:\n",
    "   - How could you modify our binary classifier to recommend products?\n",
    "   - *Your answer: ____*\n",
    "\n",
    "3. **Image Recognition**:\n",
    "   - Our network used 2 features. How many \"features\" does an image have?\n",
    "   - *Your answer: ____*\n",
    "\n",
    "4. **Scaling Up**:\n",
    "   - Our network had 4 hidden neurons. How many do you think GPT-4 has?\n",
    "   - *Your answer: ____*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Why Open Source?\n",
    "\n",
    "üîç **RESEARCH TASK 9**:\n",
    "Research the benefits of open-source AI:\n",
    "- **Community support**: Find 3 popular open-source AI libraries\n",
    "- **Reproducibility**: Why is this important in research?\n",
    "- **Transparency**: How does this help with AI safety?\n",
    "- **Cost**: Compare costs of using open-source vs. proprietary models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore some popular open-source libraries\n",
    "libraries_to_explore = {\n",
    "    'PyTorch': 'Deep learning framework by Meta',\n",
    "    'TensorFlow': 'Deep learning framework by Google', \n",
    "    'Hugging Face Transformers': 'Pre-trained language models',\n",
    "    'scikit-learn': 'Traditional machine learning',\n",
    "    'OpenCV': 'Computer vision library'\n",
    "}\n",
    "\n",
    "print(\"Popular Open-Source AI Libraries:\")\n",
    "for lib, description in libraries_to_explore.items():\n",
    "    print(f\"‚Ä¢ {lib}: {description}\")\n",
    "\n",
    "print(\"\\nüîç TODO: Research Task\")\n",
    "print(\"For each library above:\")\n",
    "print(\"1. Visit their GitHub repository\")\n",
    "print(\"2. Check how many stars and contributors they have\")\n",
    "print(\"3. Look at their documentation\")\n",
    "print(\"4. Find one interesting example or use case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reflection and Next Steps\n",
    "\n",
    "### What You've Accomplished\n",
    "‚úÖ **Implemented a neural network from scratch**\n",
    "‚úÖ **Understood forward and backward propagation**\n",
    "‚úÖ **Trained a model and evaluated its performance**\n",
    "‚úÖ **Connected theory to real-world applications**\n",
    "‚úÖ **Explored the open-source AI ecosystem**\n",
    "\n",
    "### Prepare for Day 2\n",
    "Tomorrow we'll explore:\n",
    "- **Large Language Models**: Much bigger versions of what we built today\n",
    "- **Fine-tuning**: Adapting pre-trained models for specific tasks\n",
    "- **RAG (Retrieval-Augmented Generation)**: Combining knowledge retrieval with generation\n",
    "- **Building AI Applications**: Creating user interfaces with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final challenge: Experiment with hyperparameters\n",
    "print(\"üéØ FINAL CHALLENGE:\")\n",
    "print(\"Try modifying these parameters and observe the effects:\")\n",
    "print(\"1. learning_rate: Try 0.01, 0.1, 1.0\")\n",
    "print(\"2. hidden_size: Try 2, 8, 16 neurons\")\n",
    "print(\"3. epochs: Try 500, 2000\")\n",
    "print(\"4. Add more hidden layers\")\n",
    "\n",
    "print(\"\\nüìù Questions to consider:\")\n",
    "print(\"‚Ä¢ Which combination gives the best test accuracy?\")\n",
    "print(\"‚Ä¢ How does training time change with more neurons?\")\n",
    "print(\"‚Ä¢ Can you achieve >95% accuracy?\")\n",
    "\n",
    "# TODO: Your experiments here\n",
    "# Copy the training code above and modify the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully:\n",
    "- ‚úÖ Built and trained a neural network from scratch\n",
    "- ‚úÖ Understood the mathematics behind AI\n",
    "- ‚úÖ Connected theory to real-world applications\n",
    "- ‚úÖ Explored the open-source ecosystem\n",
    "- ‚úÖ Prepared for advanced topics in Day 2\n",
    "\n",
    "**Share your insights**: What was the most surprising thing you learned?\n",
    "\n",
    "---\n",
    "*AI Refresher Complete - Ready for Day 2! üöÄ*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
